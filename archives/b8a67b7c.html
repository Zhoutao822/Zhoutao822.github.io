<!-- build time:Sat Jan 18 2020 21:25:26 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://zhoutao822.coding.me").hostname,root:"/",scheme:"Pisces",version:"7.7.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!0,preload:!0},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="道高一尺魔高一丈，魔高一尺道高一丈参考：Generative Adversarial Networks[GAN学习系列] 初识GAN[GAN学习系列2] GAN的起源令人拍案叫绝的Wasserstein GANTips and tricks to make GANs workImage-to-Image Translation with Conditional Adversarial Networ"><meta property="og:type" content="article"><meta property="og:title" content="生成式对抗网络"><meta property="og:url" content="http://zhoutao822.coding.me/archives/b8a67b7c.html"><meta property="og:site_name" content="Tao"><meta property="og:description" content="道高一尺魔高一丈，魔高一尺道高一丈参考：Generative Adversarial Networks[GAN学习系列] 初识GAN[GAN学习系列2] GAN的起源令人拍案叫绝的Wasserstein GANTips and tricks to make GANs workImage-to-Image Translation with Conditional Adversarial Networ"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://zhoutao822.coding.me/archives/b8a67b7c/1.png"><meta property="og:image" content="http://zhoutao822.coding.me/archives/b8a67b7c/2.png"><meta property="og:image" content="http://zhoutao822.coding.me/archives/b8a67b7c/3.png"><meta property="og:image" content="http://zhoutao822.coding.me/archives/b8a67b7c/4.png"><meta property="og:image" content="http://zhoutao822.coding.me/archives/b8a67b7c/5.png"><meta property="article:published_time" content="2019-01-03T13:26:22.000Z"><meta property="article:modified_time" content="2020-01-17T15:07:46.290Z"><meta property="article:author" content="Tao Zhou"><meta property="article:tag" content="Theory"><meta property="article:tag" content="GAN"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://zhoutao822.coding.me/archives/b8a67b7c/1.png"><link rel="canonical" href="http://zhoutao822.coding.me/archives/b8a67b7c.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>生成式对抗网络 | Tao</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Tao" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tao</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://zhoutao822.coding.me/archives/b8a67b7c.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Tao Zhou"><meta itemprop="description" content="学习笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tao"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">生成式对抗网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-01-03 21:26:22" itemprop="dateCreated datePublished" datetime="2019-01-03T21:26:22+08:00">2019-01-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-17 23:07:46" itemprop="dateModified" datetime="2020-01-17T23:07:46+08:00">2020-01-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span> </a></span></span><span id="/archives/b8a67b7c.html" class="post-meta-item leancloud_visitors" data-flag-title="生成式对抗网络" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/archives/b8a67b7c.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/archives/b8a67b7c.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.1k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>6 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>道高一尺魔高一丈，魔高一尺道高一丈</strong></p><p>参考：</p><blockquote><p><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Generative Adversarial Networks</a><br><a href="https://juejin.im/post/5bd07e26e51d457a7c7bbc31" target="_blank" rel="noopener">[GAN学习系列] 初识GAN</a><br><a href="https://juejin.im/post/5bdd70886fb9a049f912028d" target="_blank" rel="noopener">[GAN学习系列2] GAN的起源</a><br><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a><br><a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">Tips and tricks to make GANs work</a><br><a href="https://arxiv.org/pdf/1611.07004v1.pdf" target="_blank" rel="noopener">Image-to-Image Translation with Conditional Adversarial Networks</a></p></blockquote><p>GAN，即生成式对抗网络，是一个生成模型，也是半监督和无监督学习模型，它可以在<strong>不需要大量标注数据</strong>的情况下学习深度表征。最大的特点就是提出了一种让两个深度网络对抗训练的方法。</p><p>GAN 主要就是两个网络组成，生成器网络(Generator)和判别器网络(Discriminator)，通过这两个网络的互相博弈，让生成器网络最终能够学习到输入数据的分布，这也就是 GAN 想达到的目的：学习输入数据的分布。</p><a id="more"></a><h2 id="1-基本原理"><a href="#1-基本原理" class="headerlink" title="1. 基本原理"></a>1. 基本原理</h2><img src="/archives/b8a67b7c/1.png"><ul><li>D 是判别器，负责对输入的真实数据和由 G 生成的假数据进行判断，其输出是 0 和 1，即它本质上是一个二值分类器，目标就是对输入为真实数据输出是 1，对假数据的输入，输出是 0；</li><li>G 是生成器，它接收的是一个随机噪声，并生成图像。</li></ul><p>在训练的过程中，G 的目标是尽可能生成足够真实的数据去迷惑 D，而 D 就是要将 G 生成的图片都辨别出来，这样两者就是互相博弈，最终是要达到一个平衡，也就是纳什均衡。</p><p>需要注意的是，在原始GAN中，生成器接收的是随机噪声（大小可能在$[-1， 1]$之间），输出的是具有真实图片风格（不完全相同）的图片，也不需要标签，也就是说从噪声到图片的映射是不可描述的，而学习到的<strong>输入数据的特征</strong>仅能够作用在随机噪声上，因此原始GAN的作用被限制了，如果能将生成器的输入为某个与真实图片相关的特征映射，那么我们就可以利用这个生成器来进行图片风格迁移。</p><h2 id="2-GAN特点"><a href="#2-GAN特点" class="headerlink" title="2. GAN特点"></a>2. GAN特点</h2><p>优：</p><ul><li>对于生成式模型来说，可以类比<code>语音识别-RBM和DBN</code>以及<code>语音识别-HMM</code>，RBM和HMM都是基于隐变量状态来描述观察变量，而GAN可以不考虑隐变量而是通过神经网络来描述生成器和判别器，这也就意味着GAN不需要对状态作近似推断，也不依赖马尔可夫链，仅需要反向传播即可训练网络，相比其他生成模型（VAE、玻尔兹曼机），可以生成更好的生成样本；</li><li>理论上，只要是可微分函数都可以用于构建 D 和 G，因为能够与深度神经网络结合做深度生成式模型；</li><li>G 的参数更新不是直接来自数据样本，而是使用来自 D 的反向传播；</li><li>GAN 是一种半监督学习模型，对训练集不需要太多有标签的数据；</li><li>没有必要遵循任何种类的因子分解去设计模型，所有的生成器和鉴别器都可以正常工作。</li></ul><p>劣：</p><ul><li>可解释性差，生成模型的分布$P_g(G)$没有显式的表达；</li><li>比较难训练，D 与 G 之间需要很好的同步，例如 D 更新 k 次而 G 更新一次；</li><li>训练 GAN 需要达到纳什均衡，有时候可以用梯度下降法做到，有时候做不到，我们还没有找到很好的达到纳什均衡的方法，所以训练 GAN 相比 VAE 或者 PixelRNN 是不稳定的，但我认为在实践中它还是比训练玻尔兹曼机稳定的多；</li><li>它很难去学习生成离散的数据，就像文本；</li><li>相比玻尔兹曼机，GANs 很难根据一个像素值去猜测另外一个像素值，GANs 天生就是做一件事的，那就是一次产生所有像素，你可以用 BiGAN 来修正这个特性，它能让你像使用玻尔兹曼机一样去使用 Gibbs 采样来猜测缺失值；</li><li>训练不稳定，G 和 D 很难收敛；</li><li>训练还会遭遇梯度消失、模式崩溃的问题；</li><li>缺乏比较有效的直接可观的评估模型生成效果的方法。</li></ul><h3 id="2-1-为什么训练会出现梯度消失和模式崩溃"><a href="#2-1-为什么训练会出现梯度消失和模式崩溃" class="headerlink" title="2.1 为什么训练会出现梯度消失和模式崩溃"></a>2.1 为什么训练会出现梯度消失和模式崩溃</h3><p>梯度消失和模式奔溃其实就是这种情况下的两个结果，分别对应 D 和 G 是强大的一方的结果。</p><ol><li>对于梯度消失的情况是<strong>D 越好，G 的梯度消失越严重</strong>，因为 G 的梯度更新来自 D，而在训练初始阶段，G 的输入是随机生成的噪声，肯定不会生成很好的图片，D 会很容易就判断出来真假样本，也就是 D 的训练几乎没有损失，也就没有有效的梯度信息回传给 G 让 G 去优化自己。这样的现象叫做 gradient vanishing，梯度消失问题。</li><li>对于模式奔溃（mode collapse）问题，主要就是<strong>G 比较强，导致 D 不能很好区分出真实图片和 G 生成的假图片</strong>，而如果此时 G 其实还不能完全生成足够真实的图片的时候，但 D 却分辨不出来，并且给出了正确的评价，那么 G 就会认为这张图片是正确的，接下来就继续这么输出这张或者这些图片，然后 D 还是给出正确的评价，于是两者就是这么相互欺骗，这样 G 其实就只会输出固定的一些图片，导致的结果除了生成图片不够真实，还有就是多样性不足的问题。</li></ol><h3 id="2-2-为什么GAN不适合处理文本数据"><a href="#2-2-为什么GAN不适合处理文本数据" class="headerlink" title="2.2 为什么GAN不适合处理文本数据"></a>2.2 为什么GAN不适合处理文本数据</h3><ol><li>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设 softmax 的输出是$(0.2， 0.3， 0.1，0.2，0.15，0.05)$，那么变为 onehot是$(0，1，0，0，0，0)$，如果softmax输出是$(0.2， 0.25， 0.2， 0.1，0.15，0.1 )$，one-hot 仍然是$(0， 1， 0， 0， 0， 0)$，所以对于生成器来说，G 输出了不同的结果, 但是 D 给出了同样的判别结果，并不能将梯度更新信息很好的传递到 G 中去，所以 D 最终输出的判别没有意义。</li><li>GAN 的损失函数是 JS 散度，JS 散度不适合衡量不想交分布之间的距离。（WGAN 虽然使用 wassertein 距离代替了 JS 散度，但是在生成文本上能力还是有限，GAN 在生成文本上的应用有 seq-GAN,和强化学习结合的产物）</li></ol><h3 id="2-3-为什么GAN中的优化器不常用SGD"><a href="#2-3-为什么GAN中的优化器不常用SGD" class="headerlink" title="2.3 为什么GAN中的优化器不常用SGD"></a>2.3 为什么GAN中的优化器不常用SGD</h3><ol><li>SGD 容易震荡，容易使 GAN 的训练更加不稳定。</li><li>GAN 的目的是在高维非凸的参数空间中找到纳什均衡点，GAN 的纳什均衡点是一个鞍点，但是 SGD 只会找到局部极小值，因为 SGD 解决的是一个寻找最小值的问题，但 GAN 是一个博弈问题。</li></ol><h2 id="3-训练技巧"><a href="#3-训练技巧" class="headerlink" title="3. 训练技巧"></a>3. 训练技巧</h2><h3 id="3-1-对输入进行规范化"><a href="#3-1-对输入进行规范化" class="headerlink" title="3.1 对输入进行规范化"></a>3.1 对输入进行规范化</h3><ul><li>将输入规范化到 -1 和 1 之间</li><li>G 的输出层采用Tanh激活函数</li></ul><h3 id="3-2-采用修正的损失函数"><a href="#3-2-采用修正的损失函数" class="headerlink" title="3.2 采用修正的损失函数"></a>3.2 采用修正的损失函数</h3><p>在原始 GAN 论文中，损失函数 G 是要$\min \log(1-D)$, 但实际使用的时候是采用$\max \log(D)$，作者给出的原因是前者会导致梯度消失问题。<br>但实际上，即便是作者提出的这种实际应用的损失函数也是存在问题，即模崩溃的问题，在接下来提出的 GAN 相关的论文中，就有不少论文是针对这个问题进行改进的，如 WGAN 模型就提出一种新的损失函数。</p><h3 id="3-3-从球体上采样噪声"><a href="#3-3-从球体上采样噪声" class="headerlink" title="3.3 从球体上采样噪声"></a>3.3 从球体上采样噪声</h3><ul><li>不要采用均匀分布来采样</li><li>从高斯分布中采样得到随机噪声</li><li>当进行插值操作的时候，从大圆进行该操作，而不要直接从点 A 到 点 B 直线操作，如下图所示</li></ul><img src="/archives/b8a67b7c/2.png"><h3 id="3-4-BatchNorm"><a href="#3-4-BatchNorm" class="headerlink" title="3.4 BatchNorm"></a>3.4 BatchNorm</h3><ul><li>采用 mini-batch BatchNorm，要保证每个 mini-batch 都是真实图片或者都是生成图片</li><li>不采用 BatchNorm 的时候，可以采用 instance normalization（对每个样本的规范化操作，减均值除以标准差）</li><li>可以使用虚拟批量归一化(virtural batch normalization):开始训练之前预定义一个 batch R，对每一个新的 batch X，都使用 R+X 的级联来计算归一化参数</li></ul><h3 id="3-5-避免稀疏的梯度：ReLU、MaxPool"><a href="#3-5-避免稀疏的梯度：ReLU、MaxPool" class="headerlink" title="3.5 避免稀疏的梯度：ReLU、MaxPool"></a>3.5 避免稀疏的梯度：ReLU、MaxPool</h3><ul><li>稀疏梯度会影响 GAN 的稳定性</li><li>在 G 和 D 中采用 LeakyReLU 代替 Relu 激活函数</li><li>对于下采样操作，可以采用平均池化(Average Pooling) 和 Conv2d+stride 的替代方案</li><li>对于上采样操作，可以使用 <a href="https://arxiv.org/abs/1609.05158" target="_blank" rel="noopener">PixelShuffle</a>, ConvTranspose2d + stride</li></ul><h3 id="3-6-标签的使用"><a href="#3-6-标签的使用" class="headerlink" title="3.6 标签的使用"></a>3.6 标签的使用</h3><ul><li>标签平滑。也就是如果有两个目标标签，假设真实图片标签是 1，生成图片标签是 0，那么对每个输入例子，如果是真实图片，采用 0.7 到 1.2 之间的一个随机数字来作为标签，而不是 1；一般是采用单边标签平滑</li><li>在训练 D 的时候，偶尔翻转标签</li><li>有标签数据就尽量使用标签</li></ul><h3 id="3-7-DCGAN-混合模型"><a href="#3-7-DCGAN-混合模型" class="headerlink" title="3.7 DCGAN/混合模型"></a>3.7 DCGAN/混合模型</h3><ul><li>尽量使用DCGAN</li><li>如果DCGAN不稳定，使用混合模型：KL+GAN或者VAE+GAN</li></ul><h3 id="3-8-使用RL的稳定性tricks"><a href="#3-8-使用RL的稳定性tricks" class="headerlink" title="3.8 使用RL的稳定性tricks"></a>3.8 使用RL的稳定性tricks</h3><ul><li>训练过程回放<ul><li>缓存之前的生成器生成的结果，偶尔展示出来</li><li>保存G和D的检查点，偶尔将当前G和D置换为以前保存的G和D，以训练几轮</li></ul></li><li>All stability tricks that work for deep deterministic policy gradients</li></ul><h3 id="3-9-使用-Adam-优化器"><a href="#3-9-使用-Adam-优化器" class="headerlink" title="3.9 使用 Adam 优化器"></a>3.9 使用 Adam 优化器</h3><ul><li>遵守Adam规则</li><li>D使用SGD，G使用Adam</li></ul><h3 id="3-10-尽早追踪失败的原因"><a href="#3-10-尽早追踪失败的原因" class="headerlink" title="3.10 尽早追踪失败的原因"></a>3.10 尽早追踪失败的原因</h3><ul><li>D 的 loss 变成 0，那么这就是训练失败了</li><li>检查规范的梯度：如果超过 100，那出问题了</li><li>如果训练正常，那么 D loss 有低方差并且随着时间降低</li><li>如果 g loss 稳定下降，那么它是用糟糕的生成样本欺骗了 D</li></ul><h3 id="3-11-不要通过统计学来平衡-loss（除非你有一个好的理由）"><a href="#3-11-不要通过统计学来平衡-loss（除非你有一个好的理由）" class="headerlink" title="3.11 不要通过统计学来平衡 loss（除非你有一个好的理由）"></a>3.11 不要通过统计学来平衡 loss（除非你有一个好的理由）</h3><ul><li>不要试图寻找一个训练轮数以避免模式崩溃（无用的做法）</li><li>如果要这样做，需要制定一个准则而非依靠直觉</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while lossD &gt; A:</span><br><span class="line">  train D</span><br><span class="line">while lossG &gt; B:</span><br><span class="line">  train G</span><br></pre></td></tr></table></figure><h3 id="3-12-给输入添加噪声，随时间衰减"><a href="#3-12-给输入添加噪声，随时间衰减" class="headerlink" title="3.12 给输入添加噪声，随时间衰减"></a>3.12 给输入添加噪声，随时间衰减</h3><ul><li>给 D 的输入添加人为的噪声<ul><li><a href="https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/" target="_blank" rel="noopener">www.inference.vc/instance-no…</a></li><li><a href="https://openreview.net/forum?id=Hk4_qw5xe" target="_blank" rel="noopener">openreview.net/forum?id=Hk…</a></li></ul></li><li>给 G 的每层都添加高斯噪声</li></ul><h3 id="3-13-对于-Conditional-GANs-的离散变量"><a href="#3-13-对于-Conditional-GANs-的离散变量" class="headerlink" title="3.13 对于 Conditional GANs 的离散变量"></a>3.13 对于 Conditional GANs 的离散变量</h3><ul><li>使用一个 Embedding 层</li><li>对输入图片添加一个额外的通道</li><li>保持 embedding 低维并通过上采样操作来匹配图像的通道大小</li></ul><h3 id="3-14-在-G-的训练和测试阶段使用-Dropouts"><a href="#3-14-在-G-的训练和测试阶段使用-Dropouts" class="headerlink" title="3.14 在 G 的训练和测试阶段使用 Dropouts"></a>3.14 在 G 的训练和测试阶段使用 Dropouts</h3><ul><li>以 dropout 的形式提供噪声(50%的概率)</li><li>训练和测试阶段，在 G 的几层使用</li></ul><h2 id="4-对抗样本"><a href="#4-对抗样本" class="headerlink" title="4. 对抗样本"></a>4. 对抗样本</h2><p><strong>对抗样本(adversarial example)，它是指经过精心计算得到的用于误导分类器的样本</strong>。例如下图就是一个例子，左边是一个熊猫，但是添加了少量随机噪声变成右图后，分类器给出的预测类别却是长臂猿，但视觉上左右两幅图片并没有太大改变。</p><img src="/archives/b8a67b7c/3.png"><p>这是因为图像分类器本质上是高维空间的一个复杂的决策边界。当然涉及到图像分类的时候，由于是高维空间而不是简单的两维或者三维空间，我们无法画出这个边界出来。但是我们可以肯定的是，训练完成后，分类器是无法泛化到所有数据上，除非我们的训练集包含了分类类别的所有数据，但实际上我们做不到。而做不到泛化到所有数据的分类器，其实就会过拟合训练集的数据，这也就是我们可以利用的一点。</p><p>我们可以给图片添加一个非常接近于 0 的随机噪声，这可以通过控制噪声的 L2 范数来实现。L2 范数可以看做是一个向量的长度，这里有个诀窍就是图片的像素越多，即图片尺寸越大，其平均 L2 范数也就越大。因此，当添加的噪声的范数足够低，那么视觉上你不会觉得这张图片有什么不同，正如上述右边的图片一样，看起来依然和左边原始图片一模一样；但是，在向量空间上，添加噪声后的图片和原始图片已经有很大的距离了。</p><p>因为在 L2 范数看来，对于熊猫和长臂猿的决策边界并没有那么远，添加了非常微弱的随机噪声的图片可能就远离了熊猫的决策边界内，到达长臂猿的预测范围内，因此欺骗了分类器。</p><p>除了这种简单的添加随机噪声，还可以通过图像变形的方式，使得新图像和原始图像视觉上一样的情况下，让分类器得到有很高置信度的错误分类结果。这种过程也被称为对抗攻击(adversarial attack)，这种生成方式的简单性也是给 GAN 提供了解释。</p><h2 id="5-生成对抗网络GAN"><a href="#5-生成对抗网络GAN" class="headerlink" title="5. 生成对抗网络GAN"></a>5. 生成对抗网络GAN</h2><p>生成器G与判别器D组合起来就是GAN，其目标是</p><p>$$<br>\underset{G}{\min} \underset{D}{\max} V(D, G) = \mathbb{E}<em>{\boldsymbol{x} \sim p</em>{data}(\boldsymbol{x})}[\log D(\boldsymbol{x})] + \mathbb{E}<em>{\boldsymbol{z} \sim p</em>{\boldsymbol{z}}(\boldsymbol{z})}[\log (1 - D(G(\boldsymbol{z})))]<br>$$</p><p>对于判别器D来说，其更新梯度为</p><p>$$<br>\bigtriangledown_{\theta_d}\frac{1}{m}\sum^m_{i=1}[\log D(\boldsymbol{x}^{(i)} ) + \log(1 - D(G(\boldsymbol{z}^{(i)})))]<br>$$</p><p>对于生成器G来说，其更新梯度为</p><p>$$<br>\bigtriangledown_{\theta_g}\frac{1}{m} \sum^m_{i=1}\log(1-D(G(\boldsymbol{z}^{(i)})))<br>$$</p><p>这里根据它们的损失函数分析下，G 网络的训练目标就是让<strong>D(G(z)) 趋近于 1</strong>，这也是让其 loss 变小的做法；而 D 网络的训练目标是区分真假数据，自然是让<strong>D(x) 趋近于 1，而 D(G(z)) 趋近于 0</strong>。这就是两个网络相互对抗，彼此博弈的过程了。</p><img src="/archives/b8a67b7c/4.png"><p>上图中，黑色曲线表示输入数据 x 的实际分布，绿色曲线表示的是 G 网络生成数据的分布，我们的目标自然是希望着两条曲线可以相互重合，也就是两个数据分布一致了。而蓝色的曲线表示的是生成数据对应于 D 的分布。</p><p>在 a 图中是刚开始训练的时候，D 的分类能力还不是最好，因此有所波动，而生成数据的分布也自然和真实数据分布不同，毕竟 G 网络输入是随机生成的噪声；到了 b 图的时候，D 网络的分类能力就比较好了，可以看到对于真实数据和生成数据，它是明显可以区分出来，也就是给出的概率是不同的；</p><p>而绿色的曲线，即 G 网络的目标是学习真实数据的分布，所以它会往蓝色曲线方向移动，也就是 c 图了，并且因为 G 和 D 是相互对抗的，当 G 网络提升，也会影响 D 网络的分辨能力。论文中，Ian Goodfellow 做出了证明，当假设 G 网络不变，训练 D 网络，最优的情况会是：</p><p>$$<br>D_G^*(\boldsymbol{x}) = \frac{p_{data}(\boldsymbol{x})}{p_{data}(\boldsymbol{x}) + p_g(\boldsymbol{x})}<br>$$</p><p>也就是当生成数据的分布$p_g(x)$趋近于真实数据分布$p_{data}(x)$的时候，D 网络输出的概率$D_G^*(x)$会趋近于 0.5，也就是 d 图的结果，这也是最终希望达到的训练结果，这时候 G 和 D 网络也就达到一个平衡状态。</p><h2 id="6-算法"><a href="#6-算法" class="headerlink" title="6. 算法"></a>6. 算法</h2><p>论文给出的算法实现过程如下所示</p><img src="/archives/b8a67b7c/5.png"><p>这里包含了一些训练的技巧和方法：</p><ol><li>首先 G 和 D 是同步训练，但两者训练次数不一样，通常是<strong>D 网络训练 k 次后，G 训练一次</strong>。主要原因是 GAN 刚开始训练时候会很不稳定；</li><li>D 的训练是<strong>同时输入真实数据和生成数据来计算 loss，而不是采用交叉熵（cross entropy）分开计算</strong>。不采用 cross entropy 的原因是这会让$D(G(z))$变为 0，导致没有梯度提供给 G 更新，而现在 GAN 的做法是会收敛到 0.5；</li><li>实际训练的时候，作者是采用$-\log(D(G(\boldsymbol{z})))$来代替$\log(1-D(G(\boldsymbol{z})))$，这是希望在训练初始就可以加大梯度信息，这是因为初始阶段 D 的分类能力会远大于 G 生成足够真实数据的能力，但这种修改也将让整个 GAN 不再是一个完美的零和博弈。</li></ol></div><div class="popular-posts-header">推荐文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/7d1dcda7.html" rel="bookmark">模型评估与选择</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8ddc7426.html" rel="bookmark">决策树</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/280b588e.html" rel="bookmark">支持向量机</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/7ca31f7.html" rel="bookmark">神经网络</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8f45a2c4.html" rel="bookmark">深度学习-优化器</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tao Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://zhoutao822.coding.me/archives/b8a67b7c.html" title="生成式对抗网络">http://zhoutao822.coding.me/archives/b8a67b7c.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Theory/" rel="tag"># Theory</a> <a href="/tags/GAN/" rel="tag"># GAN</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/d178b7d8.html" rel="prev" title="深度学习-自编码器"><i class="fa fa-chevron-left"></i> 深度学习-自编码器</a></div><div class="post-nav-item"><a href="/archives/72fab9d8.html" rel="next" title="集成学习">集成学习 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-基本原理"><span class="nav-text">1. 基本原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-GAN特点"><span class="nav-text">2. GAN特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-为什么训练会出现梯度消失和模式崩溃"><span class="nav-text">2.1 为什么训练会出现梯度消失和模式崩溃</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-为什么GAN不适合处理文本数据"><span class="nav-text">2.2 为什么GAN不适合处理文本数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-为什么GAN中的优化器不常用SGD"><span class="nav-text">2.3 为什么GAN中的优化器不常用SGD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-训练技巧"><span class="nav-text">3. 训练技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-对输入进行规范化"><span class="nav-text">3.1 对输入进行规范化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-采用修正的损失函数"><span class="nav-text">3.2 采用修正的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-从球体上采样噪声"><span class="nav-text">3.3 从球体上采样噪声</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-BatchNorm"><span class="nav-text">3.4 BatchNorm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-避免稀疏的梯度：ReLU、MaxPool"><span class="nav-text">3.5 避免稀疏的梯度：ReLU、MaxPool</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-标签的使用"><span class="nav-text">3.6 标签的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-7-DCGAN-混合模型"><span class="nav-text">3.7 DCGAN&#x2F;混合模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-8-使用RL的稳定性tricks"><span class="nav-text">3.8 使用RL的稳定性tricks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-9-使用-Adam-优化器"><span class="nav-text">3.9 使用 Adam 优化器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-10-尽早追踪失败的原因"><span class="nav-text">3.10 尽早追踪失败的原因</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-11-不要通过统计学来平衡-loss（除非你有一个好的理由）"><span class="nav-text">3.11 不要通过统计学来平衡 loss（除非你有一个好的理由）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-12-给输入添加噪声，随时间衰减"><span class="nav-text">3.12 给输入添加噪声，随时间衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-13-对于-Conditional-GANs-的离散变量"><span class="nav-text">3.13 对于 Conditional GANs 的离散变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-14-在-G-的训练和测试阶段使用-Dropouts"><span class="nav-text">3.14 在 G 的训练和测试阶段使用 Dropouts</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-对抗样本"><span class="nav-text">4. 对抗样本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-生成对抗网络GAN"><span class="nav-text">5. 生成对抗网络GAN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-算法"><span class="nav-text">6. 算法</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Tao Zhou" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Tao Zhou</p><div class="site-description" itemprop="description">学习笔记</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">61</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">126</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Tao Zhou</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">897k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">13:36</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0</div><script>function leancloudSelector(url) {
    url = encodeURI(url);
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.getAttribute('id'));
      var title = visitors.getAttribute('data-flag-title');

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
              leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .catch(error => {
                console.error('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.getAttribute('id'));
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (let item of results) {
            let { url, time } = item;
            leancloudSelector(url).innerText = time;
          }
          for (let url of entries) {
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
            'X-LC-Key': 'xoukWFyqvFIXJ6DfxLLYtsTP',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
      appKey: 'xoukWFyqvFIXJ6DfxLLYtsTP',
      placeholder: "Just go go",
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: false,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html><!-- rebuild by neat -->