<!-- build time:Sun Jan 19 2020 08:43:27 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://zhoutao822.coding.me").hostname,root:"/",scheme:"Pisces",version:"7.7.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!0,preload:!0},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="1. 数据集说明1.1 线性回归数据集-Boston房价 数据来源：sklearn.datasets.load_boston（tensorflow.keras.datasets.boston_housing理论上应该是一模一样的）； 数据集形状：总计506个样本，每个样本由14个属性表示，一般将最后一个房价作为target，所有属性值均为number，详情可调用load_boston()[&amp;#39"><meta property="og:type" content="article"><meta property="og:title" content="线性模型-coding"><meta property="og:url" content="http://zhoutao822.coding.me/archives/c516f6f0.html"><meta property="og:site_name" content="Tao"><meta property="og:description" content="1. 数据集说明1.1 线性回归数据集-Boston房价 数据来源：sklearn.datasets.load_boston（tensorflow.keras.datasets.boston_housing理论上应该是一模一样的）； 数据集形状：总计506个样本，每个样本由14个属性表示，一般将最后一个房价作为target，所有属性值均为number，详情可调用load_boston()[&amp;#39"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/boston.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/standpre.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/lwlrpre.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/loss.png"><meta property="article:published_time" content="2018-11-07T14:00:56.000Z"><meta property="article:modified_time" content="2020-01-18T14:47:13.364Z"><meta property="article:author" content="Tao Zhou"><meta property="article:tag" content="Linear Model"><meta property="article:tag" content="Code"><meta property="article:tag" content="LDA"><meta property="article:tag" content="LinearClassifier"><meta property="article:tag" content="LinearRegressor"><meta property="article:tag" content="Estimator"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://114.116.9.65:7777/images/2020/01/18/boston.png"><link rel="canonical" href="http://zhoutao822.coding.me/archives/c516f6f0.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>线性模型-coding | Tao</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Tao" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tao</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://zhoutao822.coding.me/archives/c516f6f0.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Tao Zhou"><meta itemprop="description" content="学习笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tao"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">线性模型-coding</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-11-07 22:00:56" itemprop="dateCreated datePublished" datetime="2018-11-07T22:00:56+08:00">2018-11-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-18 22:47:13" itemprop="dateModified" datetime="2020-01-18T22:47:13+08:00">2020-01-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Coding/" itemprop="url" rel="index"><span itemprop="name">Coding</span> </a></span></span><span id="/archives/c516f6f0.html" class="post-meta-item leancloud_visitors" data-flag-title="线性模型-coding" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/archives/c516f6f0.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/archives/c516f6f0.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>15k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>13 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-数据集说明"><a href="#1-数据集说明" class="headerlink" title="1. 数据集说明"></a>1. 数据集说明</h2><h3 id="1-1-线性回归数据集-Boston房价"><a href="#1-1-线性回归数据集-Boston房价" class="headerlink" title="1.1 线性回归数据集-Boston房价"></a>1.1 线性回归数据集-Boston房价</h3><ul><li>数据来源：<code>sklearn.datasets.load_boston</code>（<code>tensorflow.keras.datasets.boston_housing</code>理论上应该是一模一样的）；</li><li>数据集形状：总计506个样本，每个样本由14个属性表示，一般将最后一个房价作为target，所有属性值均为number，详情可调用<code>load_boston()[&#39;DESCR&#39;]</code>了解每个属性的具体含义；</li><li>数据集划分：随机选出20%数据作为测试集，不做验证集要求；</li><li>性能度量：MSE或者RMSE均可以。</li></ul><h3 id="1-2-二分类数据集-乳腺癌"><a href="#1-2-二分类数据集-乳腺癌" class="headerlink" title="1.2 二分类数据集-乳腺癌"></a>1.2 二分类数据集-乳腺癌</h3><ul><li>数据来源：<code>sklearn.datasets.load_breast_cancer</code>；</li><li>数据集形状：总计569个样本，良性357个，恶性212个，每个样本由30个属性表示，target表示肿瘤良性1还是恶性0，所有属性值均为number，详情可调用<code>load_breast_cancer()[&#39;DESCR&#39;]</code>了解每个属性的具体含义；</li><li>数据集划分：随机选出20%数据作为测试集，不做验证集要求；</li><li>性能度量：accuracy或ROC。</li></ul><h3 id="1-3-多分类数据集-鸢尾花"><a href="#1-3-多分类数据集-鸢尾花" class="headerlink" title="1.3 多分类数据集-鸢尾花"></a>1.3 多分类数据集-鸢尾花</h3><ul><li>数据来源：<code>sklearn.datasets.load_iris</code>；</li><li>数据集形状：总计150个样本，一共3种花，每种50个，每个样本由4个属性表示，target表示花的种类0/1/2，所有属性值均为number，详情可调用<code>load_iris()[&#39;DESCR&#39;]</code>了解每个属性的具体含义；</li><li>数据集划分：随机选出20%数据作为测试集，不做验证集要求；</li><li>性能度量：accuracy。</li></ul><a id="more"></a><h2 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2. 线性回归"></a>2. 线性回归</h2><h3 id="2-1-公式法"><a href="#2-1-公式法" class="headerlink" title="2.1 公式法"></a>2.1 公式法</h3><p><strong>我们先不考虑特征工程，仅将所有特征放入线性回归模型中</strong>。</p><p>首先导入需要的第三方库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston <span class="comment">#数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing <span class="comment">#归一化处理</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment">#数据集划分</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#观察数据集</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#绘制图表</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#数据处理</span></span><br><span class="line">pd.set_option(<span class="string">'precision'</span>, <span class="number">2</span>) <span class="comment">#设置pandas显示数据保留两位小数</span></span><br></pre></td></tr></table></figure><p>然后看看数据的大致范围与一些统计信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">boston = load_boston() <span class="comment">#加载数据，load_boston()返回的是一个字典</span></span><br><span class="line">print(boston[<span class="string">'DESCR'</span>]) <span class="comment">#打印数据集描述信息</span></span><br><span class="line"></span><br><span class="line">filepath = boston[<span class="string">'filename'</span>] <span class="comment">#调用load_boston()会下载数据集csv文件到本地，通过filename获取路径</span></span><br><span class="line">df = pd.read_csv(filepath, skiprows=<span class="number">0</span>, header=<span class="number">1</span>) <span class="comment">#通过pandas读取csv文件，由于sklearn下载的csv文件第0行是样例数和属性数，第1行是属性名称，从第2行开始才是数据，所以设置skiprows跳过第0行，设置header特征行为1</span></span><br><span class="line">df.describe() <span class="comment">#显示数据集统计信息</span></span><br></pre></td></tr></table></figure><p><img src="http://114.116.9.65:7777/images/2020/01/18/boston.png" alt="boston.png"></p><p>数据集划分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data = boston[<span class="string">'data'</span>] <span class="comment">#data对应前13列，即特征列，获取到的数据类型为np.array</span></span><br><span class="line">target = boston[<span class="string">'target'</span>] <span class="comment">#target对应最后一列，即目标列</span></span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>) <span class="comment">#调用train_test_split划分数据集，指定test_size为0.2，指定shuffle为True，在划分前打乱数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注释掉的部分是对数据进行归一化处理，减去均值，再除以标准差</span></span><br><span class="line"><span class="comment"># scaler = preprocessing.StandardScaler().fit(x_train)</span></span><br><span class="line"><span class="comment"># x_train_scale = scaler.transform(x_train)</span></span><br><span class="line"><span class="comment"># x_test_scale = scaler.transform(x_test)</span></span><br><span class="line"><span class="comment"># x_train_scale = np.column_stack((x_train_scale, np.ones(len(x_train_scale))))</span></span><br><span class="line"><span class="comment"># x_test_scale = np.column_stack((x_test_scale, np.ones(len(x_test_scale))))</span></span><br><span class="line"></span><br><span class="line">x_train = np.column_stack((x_train, np.ones(len(x_train)))) <span class="comment">#在公式法中我们还要增加一列全1为偏差bias</span></span><br><span class="line">x_test = np.column_stack((x_test, np.ones(len(x_test))))</span><br></pre></td></tr></table></figure><p>计算预测值与损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standLR</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        根据公式计算参数w（已经包括bias）</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    xMat = np.mat(x) <span class="comment">#将np.array数据转成矩阵便于后续计算</span></span><br><span class="line">    yMat = np.mat(y).T <span class="comment">#对应一列</span></span><br><span class="line"></span><br><span class="line">    xTx = xMat.T * xMat <span class="comment">#.T实现矩阵转置</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>: <span class="comment">#如果矩阵行列式为0说明矩阵不可逆</span></span><br><span class="line">        print(<span class="string">'矩阵不可逆，请使用其他方法！！'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    w = xTx.I * xMat.T * yMat <span class="comment">#计算w，w的形状是一列</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x, w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mat(x) * w <span class="comment">#根据w计算预测值，预测值也是一列</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse</span><span class="params">(pre, y)</span>:</span></span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    yMat = np.mat(y).T</span><br><span class="line">    loss = np.sum(np.square(pre - yMat)) / m <span class="comment">#计算MSE，也可以开方获取RMSE</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">w = standLR(x_train, y_train)</span><br><span class="line">pre = predict(x_test, w)</span><br><span class="line">loss = mse(pre, y_test)</span><br><span class="line">print(<span class="string">'MSE for testSet is: &#123;:.3f&#125;'</span>.format(loss))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制预测值与真实值，以y=x为标准，越接近这条线越准确</span></span><br><span class="line">plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">60</span>], [<span class="number">0</span>, <span class="number">60</span>])</span><br><span class="line">plt.scatter(pre.A, y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>MSE for testSet is: 26.741 ，每次结果都不一定相同</p></blockquote><p><img src="http://114.116.9.65:7777/images/2020/01/18/standpre.png" alt="standpre.png"></p><ul><li>当使用了归一化后的特征列数据进行求解时，我们最后得到的预测性能和没有使用归一化的是几乎一样的，但是这是不是意味着归一化没有用处呢，当然不是；</li><li>在不考虑特征工程的情形下，我们仅通过线性回归能得到的完美解$w$，在测试集上的损失是26.741，我们要想提高性能减少损失需要考虑特征工程或其他方法；</li><li>当样例数和特征数增大时，矩阵计算需要大量内存，这个方法不合适；</li><li>在标准线性回归中，我们没有考虑单个样本损失的权重，比如测试点与某些样本点距离很近，那么这些近距离的样本点的损失对测试点就应当更重要，所以它们的损失权重应该较大，而那些远离测试点的样本点，其权重应当较小，基于这个理论，我们使用局部加权线性回归测试一下。</li></ul><h3 id="2-2-局部加权线性回归LWLR"><a href="#2-2-局部加权线性回归LWLR" class="headerlink" title="2.2 局部加权线性回归LWLR"></a>2.2 局部加权线性回归LWLR</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span><span class="params">(x_point, x, y, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Description：</span></span><br><span class="line"><span class="string">            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)</span></span><br><span class="line"><span class="string">            理解：x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。</span></span><br><span class="line"><span class="string">            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。</span></span><br><span class="line"><span class="string">            算法思路：假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，</span></span><br><span class="line"><span class="string">            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    xMat = np.mat(x)</span><br><span class="line">    yMat = np.mat(y).T</span><br><span class="line">    x_point = np.mat(x_point)</span><br><span class="line">    m = np.shape(xMat)[<span class="number">0</span>]</span><br><span class="line">    weights = np.mat(np.eye(m))     <span class="comment"># eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        diff = x_point - xMat[j, :]         <span class="comment"># 计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值</span></span><br><span class="line">        <span class="comment"># print(diff * diff.T)</span></span><br><span class="line">        weights[j, j] = np.exp(diff * diff.T / (<span class="number">-2.0</span> * k**<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># print(weights[j, j])</span></span><br><span class="line">    xTx = xMat.T * (weights * xMat)     <span class="comment"># 根据矩阵乘法计算 xTx ，其中的 weights 矩阵是样本点对应的权重矩阵</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>: <span class="comment">#如果矩阵行列式为0说明矩阵不可逆</span></span><br><span class="line">        print(<span class="string">'矩阵不可逆，请使用其他方法！！'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    w = xTx.I * (xMat.T * (weights * yMat))</span><br><span class="line">    <span class="keyword">return</span> x_point * w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrPre</span><span class="params">(x_test, x, y, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    m = x_test.shape[<span class="number">0</span>]</span><br><span class="line">    pre = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        pre[i] = lwlr(x_test[i], x, y, k)</span><br><span class="line">    <span class="keyword">return</span> pre</span><br><span class="line"><span class="comment"># 这里使用了归一化后的数据，因为没有归一化的样本的diff值很大，导致exp运算后的值接近0，最后导致矩阵行列式为0</span></span><br><span class="line">pre = lwlrPre(x_test_scale, x_train_scale, y_train, k=<span class="number">1.1</span>)</span><br><span class="line">loss = mse(pre, y_test)</span><br><span class="line">print(<span class="string">'MSE for testSet is: &#123;:.3f&#125;'</span>.format(loss))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">60</span>], [<span class="number">0</span>, <span class="number">60</span>])</span><br><span class="line">plt.scatter(pre.A, y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>MSE for testSet is: 7.435 ，每次结果都不一定相同，但是效果明显优于标准线性回归</p></blockquote><p><img src="http://114.116.9.65:7777/images/2020/01/18/lwlrpre.png" alt="lwlrpre.png"></p><h3 id="2-3-岭回归"><a href="#2-3-岭回归" class="headerlink" title="2.3 岭回归"></a>2.3 岭回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeRegress</span><span class="params">(x, y, lam=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Desc：</span></span><br><span class="line"><span class="string">            这个函数实现了给定 lambda 下的岭回归求解。</span></span><br><span class="line"><span class="string">            如果数据的特征比样本点还多，就不能再使用上面介绍的的线性回归和局部现行回归了，因为计算 (xTx)^(-1)会出现错误。</span></span><br><span class="line"><span class="string">            如果特征比样本点还多（n &gt; m），也就是说，输入数据的矩阵x不是满秩矩阵。非满秩矩阵在求逆时会出现问题。</span></span><br><span class="line"><span class="string">            为了解决这个问题，我们下边讲一下：岭回归，这是我们要讲的第一种缩减方法。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    xMat = np.mat(x)</span><br><span class="line">    yMat = np.mat(y).T</span><br><span class="line">    xTx = xMat.T * xMat</span><br><span class="line">    demon = xTx + np.eye(xMat.shape[<span class="number">1</span>]) * lam     <span class="comment"># 岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>: <span class="comment">#如果矩阵行列式为0说明矩阵不可逆</span></span><br><span class="line">        print(<span class="string">'矩阵不可逆，请使用其他方法！！'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    w = xTx.I * (xMat.T * yMat)</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"><span class="comment"># 由于我们使用的数据集不存在特征比样本点还多的情况，因此岭回归未起作用</span></span><br><span class="line">w = ridgeRegress(x_train_scale, y_train, lam=<span class="number">0.2</span>)</span><br><span class="line">pre = predict(x_test_scale, w)</span><br><span class="line">loss = mse(pre, y_test)</span><br><span class="line">print(loss)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">60</span>], [<span class="number">0</span>, <span class="number">60</span>])</span><br><span class="line">plt.scatter(pre.A, y_test)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>MSE for testSet is: 26.741 ，每次结果都不一定相同</p></blockquote><h3 id="2-4-Estimator"><a href="#2-4-Estimator" class="headerlink" title="2.4 Estimator"></a>2.4 Estimator</h3><p>使用梯度下降可以避免公式法面临的内存消耗问题，同时采用归一化，归一化是为了保证在梯度下降时各个参数下降步长基本同步，达到同时收敛的效果</p><p><strong>基于Tensorflow框架实现线性回归，使用Tensorflow提供的LinearRegressor</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="comment"># 默认划分20%的测试集</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = boston_housing.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注释掉的部分对数据进行归一化处理，可以加速收敛</span></span><br><span class="line"><span class="comment"># scaler = preprocessing.StandardScaler().fit(x_train)</span></span><br><span class="line"><span class="comment"># x_train = scaler.transform(x_train)</span></span><br><span class="line"><span class="comment"># x_test = scaler.transform(x_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是为了适配LinearRegressor需要的feature_columns，而feature_columns指定了参与训练的特征，我们可以直接增加或减少feature_columns来比较在不同特征数下模型的性能</span></span><br><span class="line"><span class="comment"># 把每一列数据保存为一个键值对，键的名称来源数据集说明</span></span><br><span class="line">column_names = [<span class="string">'CRIM'</span>, <span class="string">'ZN'</span>, <span class="string">'INDUS'</span>, <span class="string">'CHAS'</span>, <span class="string">'NOX'</span>, <span class="string">'RM'</span>, <span class="string">'AGE'</span>, <span class="string">'DIS'</span>, <span class="string">'RAD'</span>,</span><br><span class="line">                <span class="string">'TAX'</span>, <span class="string">'PTRATIO'</span>, <span class="string">'B'</span>, <span class="string">'LSTAT'</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDict</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123; column_names[i]: X[:, i].ravel() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(column_names))&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这里控制参与训练的特征列</span></span><br><span class="line"><span class="comment"># 终极小tips，feature_columns的key不能包含空格在名称中，否则报错not valid scope name</span></span><br><span class="line">feature_columns = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> column_names:</span><br><span class="line">    feature_columns.append(tf.feature_column.numeric_column(key=key))</span><br><span class="line"></span><br><span class="line"><span class="comment"># estimator的输入数据是一个dataset模式，具体可以上官网了解</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_train</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((createDict(x_train), y_train))</span><br><span class="line">    dataset = dataset.shuffle(<span class="number">1000</span>).batch(<span class="number">64</span>).repeat() <span class="comment"># 数据集打乱/batch/重复</span></span><br><span class="line">    <span class="keyword">return</span> dataset.make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_test</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((createDict(x_test), y_test))</span><br><span class="line">    dataset = dataset.shuffle(<span class="number">1000</span>).batch(<span class="number">64</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset.make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># LinearRegressor的参数还可以指定优化器、参数正则化等等</span></span><br><span class="line">model = tf.estimator.LinearRegressor(</span><br><span class="line">    feature_columns=feature_columns, <span class="comment"># 指定特征列</span></span><br><span class="line">    model_dir=<span class="string">"C://Users//Admin//Desktop//model"</span>, <span class="comment"># 指定模型保存的位置，包括了checkpoint和tensorboard数据</span></span><br><span class="line">    optimizer=tf.train.FtrlOptimizer(</span><br><span class="line">      learning_rate=<span class="number">0.1</span>,</span><br><span class="line">      l1_regularization_strength=<span class="number">0.001</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">model.train(input_fn=input_train, steps=<span class="number">20000</span>) <span class="comment"># 开始训练模型，steps指定训练次数，每一次消耗一个batch的数据，进行一次参数更新</span></span><br><span class="line"></span><br><span class="line">model.evaluate(input_fn=input_test) <span class="comment"># 使用测试集数据评估模型性能，若使用的是同一组训练集和测试集，那么梯度下降最终得到的损失应该不低于直接用公式法得到的损失</span></span><br></pre></td></tr></table></figure><p><strong>不使用归一化</strong>：</p><blockquote><p>‘average_loss’: 26.594913<br>‘label/mean’: 23.078432<br>‘prediction/mean’: 23.884363</p></blockquote><p><strong>使用归一化</strong>：</p><blockquote><p>‘average_loss’: 23.19075<br>‘label/mean’: 23.078432<br>‘prediction/mean’: 23.093945</p></blockquote><p><img src="http://114.116.9.65:7777/images/2020/01/18/loss.png" alt="loss.png"></p><h2 id="3-二分类sigmoid"><a href="#3-二分类sigmoid" class="headerlink" title="3. 二分类sigmoid"></a>3. 二分类sigmoid</h2><h3 id="3-1-简单梯度下降"><a href="#3-1-简单梯度下降" class="headerlink" title="3.1 简单梯度下降"></a>3.1 简单梯度下降</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">rawData = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">data = rawData[<span class="string">'data'</span>]</span><br><span class="line">target = rawData[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">x_train = np.column_stack((x_train, np.ones(len(x_train))))</span><br><span class="line">x_test = np.column_stack((x_test, np.ones(len(x_test))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数存在上溢和下溢问题</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standGrad</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    xMat = np.mat(x)</span><br><span class="line">    yMat = np.mat(y).T</span><br><span class="line">    n = xMat.shape[<span class="number">1</span>]</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    steps = <span class="number">1000</span></span><br><span class="line">    weights = np.ones((n, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 关键部分，根据迭代次数steps，每次迭代都使用全部数据，公式计算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">        pre = sigmoid(xMat * weights)</span><br><span class="line">        error = pre - yMat</span><br><span class="line">        weights -= alpha * xMat.T * error</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数计算得到预测值为1的概率，若概率大于0.5（也可以设置为其他值，</span></span><br><span class="line"><span class="comment"># 避免类别不均衡问题），则认为预测值为1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x, w)</span>:</span></span><br><span class="line">    xMat = np.mat(x)</span><br><span class="line">    pro = sigmoid(xMat * w) </span><br><span class="line">    pre = [<span class="number">1</span> <span class="keyword">if</span> p &gt; <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> pro]</span><br><span class="line">    <span class="keyword">return</span> pro, pre</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(pre, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(np.equal(pre, y).astype(np.float))/len(pre)</span><br><span class="line"></span><br><span class="line">w = standGrad(x_train, y_train)</span><br><span class="line">pro, pre = predict(x_test, w)</span><br><span class="line">print(<span class="string">'Testset prediction accuracy: &#123;:.3f&#125;'</span>.format(accuracy(pre, y_test)))</span><br></pre></td></tr></table></figure><blockquote><p>Testset prediction accuracy: 0.763</p></blockquote><h3 id="3-2-随机梯度下降"><a href="#3-2-随机梯度下降" class="headerlink" title="3.2 随机梯度下降"></a>3.2 随机梯度下降</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGrad</span><span class="params">(x, y, steps=<span class="number">300</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        随机梯度下降随机选取一个样本，针对每一个样本都进行参数的梯度下降，同时对训练参数进行约束，</span></span><br><span class="line"><span class="string">        使其在开始阶段较大拥有较大的步伐，在最终阶段拥有较小的步伐</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m, n = x.shape</span><br><span class="line">    weights = np.ones(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">        index = list(range(m))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            rand_index = int(np.random.uniform(<span class="number">0</span>, len(index)))</span><br><span class="line">            alpha = <span class="number">4</span> / (<span class="number">1.0</span> + i + j) + <span class="number">0.01</span> <span class="comment"># alpha约束，也可以使用其他公式</span></span><br><span class="line">            pre = sigmoid(np.sum(x[index[rand_index]] * weights))</span><br><span class="line">            error = pre - y[index[rand_index]]</span><br><span class="line">            weights -= alpha * error * x[index[rand_index]]</span><br><span class="line">            <span class="keyword">del</span>(index[rand_index])</span><br><span class="line">    <span class="keyword">return</span> np.mat(weights).T</span><br><span class="line"></span><br><span class="line">w = stocGrad(x_train, y_train)</span><br><span class="line">pro, pre = predict(x_test, w)</span><br><span class="line">print(<span class="string">'Testset prediction accuracy: &#123;:.3f&#125;'</span>.format(accuracy(pre, y_test)))</span><br></pre></td></tr></table></figure><blockquote><p>Testset prediction accuracy: 0.851</p></blockquote><h3 id="3-3-Estimator"><a href="#3-3-Estimator" class="headerlink" title="3.3 Estimator"></a>3.3 Estimator</h3><p><strong>基于Tensorflow框架实现二分类，使用Tensorflow提供的LinearClassifier</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">rawData = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">data = rawData[<span class="string">'data'</span>]</span><br><span class="line">target = rawData[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 终极小tips，feature_columns的key不能包含空格在名称中，否则报错not valid scope name</span></span><br><span class="line">column_names = [name.replace(<span class="string">' '</span>, <span class="string">''</span>) <span class="keyword">for</span> name <span class="keyword">in</span> rawData[<span class="string">'feature_names'</span>]]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDict</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;column_names[i]: X[:, i].ravel() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(column_names))&#125;</span><br><span class="line"></span><br><span class="line">feature_columns = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> column_names:</span><br><span class="line">    feature_columns.append(tf.feature_column.numeric_column(key=key))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用一个函数代替input_train和input_test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(x, y, training=True)</span>:</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((createDict(x), y))</span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">        dataset = dataset.shuffle(<span class="number">1000</span>).batch(<span class="number">32</span>).repeat()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataset = dataset.batch(<span class="number">32</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset.make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line">model = tf.estimator.LinearClassifier(</span><br><span class="line">    n_classes=<span class="number">2</span>, <span class="comment"># 默认为2，可以不写，其他分类需要指定</span></span><br><span class="line">    feature_columns=feature_columns, <span class="comment"># 指定特征列</span></span><br><span class="line">    model_dir=<span class="string">"C://Users//Admin//Desktop//model//classifier"</span>, <span class="comment"># 指定模型保存的位置，包括了checkpoint和tensorboard数据</span></span><br><span class="line">    optimizer=tf.train.FtrlOptimizer(</span><br><span class="line">      learning_rate=<span class="number">0.1</span>,</span><br><span class="line">      l1_regularization_strength=<span class="number">0.001</span> <span class="comment"># 增加l1正则化，系数0.001，使参数中产生更多的0，可以提高泛化性能</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">model.train(input_fn=<span class="keyword">lambda</span>: input_fn(x_train, y_train), steps=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">model.evaluate(input_fn=<span class="keyword">lambda</span>: input_fn(x_test, y_test, training=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure><blockquote><p>‘accuracy’: 0.95614034<br>‘accuracy_baseline’: 0.65789473<br>‘auc’: 0.9890598<br>‘auc_precision_recall’: 0.99319196<br>‘average_loss’: 0.10730305<br>‘label/mean’: 0.65789473<br>‘loss’: 3.058137<br>‘precision’: 0.972973<br>‘prediction/mean’: 0.6475287<br>‘recall’: 0.96<br>‘global_step’: 10000</p></blockquote><h2 id="4-多分类softmax"><a href="#4-多分类softmax" class="headerlink" title="4. 多分类softmax"></a>4. 多分类softmax</h2><p><strong>基于Tensorflow框架实现多分类，使用Tensorflow提供的eager模式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%%</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, absolute_import, division</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution() <span class="comment">#启用eager模式</span></span><br><span class="line">tfe = tf.contrib.eager</span><br><span class="line">print(<span class="string">'Tensorflow version: '</span>, tf.VERSION)</span><br><span class="line">print(<span class="string">'Eager mode: '</span>, tf.executing_eagerly())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_steps = <span class="number">10000</span></span><br><span class="line">display_step = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据，划分数据集</span></span><br><span class="line">(data, target) = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成dataset，注意这里获取到的x_train和x_test都是float64的数据，tf.matmul不能计算float64的数据，需要转成32位</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), y_train)).shuffle(<span class="number">1000</span>).batch(batch_size)</span><br><span class="line"><span class="comment"># eager模式下的迭代器</span></span><br><span class="line">dataset_iter = tfe.Iterator(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存参数，初始化为0</span></span><br><span class="line">W = tfe.Variable(tf.zeros([<span class="number">4</span>, <span class="number">3</span>]), name=<span class="string">'weights'</span>)</span><br><span class="line">b = tfe.Variable(tf.zeros([<span class="number">3</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(x, W) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失，注意这里传入的参数包括了inference_fn，即回归计算方程，便于调整regression()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(inference_fn, x, y)</span>:</span></span><br><span class="line">    <span class="comment"># sparse_softmax_cross_entropy_with_logits计算softmax并且计算交叉熵，所以只需要传入线性模型计算得到的结果就行了</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=inference_fn(x), labels=y))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(inference_fn, x, y)</span>:</span></span><br><span class="line">    pro = tf.nn.softmax(inference_fn(x))</span><br><span class="line">    <span class="comment"># softmax返回的是该样本属于各个种类的概率，这里用argmax取概率最大的index，然后与target对比</span></span><br><span class="line">    pre = tf.equal(tf.argmax(pro, <span class="number">1</span>), y)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.cast(pre, tf.float32))</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line">grad = tfe.implicit_gradients(loss)</span><br><span class="line"></span><br><span class="line">avg_loss = <span class="number">0.</span></span><br><span class="line">avg_acc = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># eager模式下迭代器到最后的时候需要重新初始化，继续取数据</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        dataset_iter = tfe.Iterator(dataset)</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line"></span><br><span class="line">    x_batch = d[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 这里需要转成int64是由于argmax返回的默认数据是int64类型</span></span><br><span class="line">    y_batch = tf.cast(d[<span class="number">1</span>], tf.int64)</span><br><span class="line"></span><br><span class="line">    batch_loss = loss(regression, x_batch, y_batch)</span><br><span class="line">    avg_loss += batch_loss</span><br><span class="line">    batch_acc = accuracy(regression, x_batch, y_batch)</span><br><span class="line">    avg_acc += batch_acc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Initial loss = &#123;:.5f&#125;'</span>.format(avg_loss))</span><br><span class="line">    <span class="comment"># 这里执行了梯度下降并更新参数</span></span><br><span class="line">    optimizer.apply_gradients(grad(regression, x_batch, y_batch))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span> ) % display_step == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">            avg_loss /= display_step</span><br><span class="line">            avg_acc /= display_step</span><br><span class="line">        print(<span class="string">'Step:&#123;:04d&#125;'</span>.format(i + <span class="number">1</span>), <span class="string">'loss = &#123;:.5f&#125;'</span>.format(avg_loss),</span><br><span class="line">        <span class="string">'accuracy = &#123;:.4f&#125;'</span>.format(avg_acc))</span><br><span class="line">        avg_acc = <span class="number">0.</span></span><br><span class="line">        avg_loss = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">test_acc = accuracy(regression, tf.cast(x_test, tf.float32), y_test)</span><br><span class="line">print(<span class="string">'Test accuracy: &#123;:.4f&#125;'</span>.format(test_acc))</span><br></pre></td></tr></table></figure><blockquote><p>Step:10000 loss = 0.14437 accuracy = 0.9738<br>Test accuracy: 1.0000</p></blockquote><h2 id="5-LDA分类-降维"><a href="#5-LDA分类-降维" class="headerlink" title="5. LDA分类/降维"></a>5. LDA分类/降维</h2><p>二分类LDA，数据集使用sklearn.datasets.load_breast_cancer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">(data, target) = load_breast_cancer(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本分类</span></span><br><span class="line">X0 = np.array([x_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_train)) <span class="keyword">if</span> y_train[i] == <span class="number">0</span>])</span><br><span class="line">X1 = np.array([x_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_train)) <span class="keyword">if</span> y_train[i] == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算样本中心</span></span><br><span class="line">mu0 = np.mean(X0, axis=<span class="number">0</span>)</span><br><span class="line">mu1 = np.mean(X1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算协方差矩阵</span></span><br><span class="line">sigma0 = np.mat(np.zeros((X0.shape[<span class="number">1</span>], X0.shape[<span class="number">1</span>]))) </span><br><span class="line">sigma1 = np.mat(np.zeros((X1.shape[<span class="number">1</span>], X1.shape[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X0.shape[<span class="number">0</span>]):</span><br><span class="line">    sigma0 += np.mat(X0[i] - mu0).T * np.mat(X0[i] - mu0)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(X1.shape[<span class="number">0</span>]):</span><br><span class="line">    sigma1 += np.mat(X1[i] - mu1).T * np.mat(X1[i] - mu1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类间散度矩阵</span></span><br><span class="line">Sw = sigma0 + sigma1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按公式计算参数w</span></span><br><span class="line">w = Sw.I * np.mat(mu0 - mu1).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算映射后的中心坐标</span></span><br><span class="line">center0 = (np.mat(mu0) * w).getA()</span><br><span class="line">center1 = (np.mat(mu1) * w).getA()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照样本距离哪个中心更近进行预测</span></span><br><span class="line">result = []</span><br><span class="line">pre = np.mat(x_test) * w</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> pre:</span><br><span class="line">    <span class="keyword">if</span> abs(p - center0) &gt; abs(p - center1):</span><br><span class="line">        result.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result.append(<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'Test accuracy: &#123;:.4f&#125;'</span>.format(np.mean(np.equal(result, y_test).astype(np.float))))</span><br></pre></td></tr></table></figure><blockquote><p>Test accuracy: 0.9123</p></blockquote><p>也可以使用<code>sklearn.discriminant_analysis.LinearDiscriminantAnalysis</code></p></div><div class="popular-posts-header">推荐文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/2d6320ff.html" rel="bookmark">神经网络-coding</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/467d5b64.html" rel="bookmark">卷积神经网络-coding</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/40b4de0.html" rel="bookmark">TensorFlow-CIFAR10</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/120687ac.html" rel="bookmark">决策树-coding</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/28da5216.html" rel="bookmark">循环神经网络-coding</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tao Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://zhoutao822.coding.me/archives/c516f6f0.html" title="线性模型-coding">http://zhoutao822.coding.me/archives/c516f6f0.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Linear-Model/" rel="tag"># Linear Model</a> <a href="/tags/Code/" rel="tag"># Code</a> <a href="/tags/LDA/" rel="tag"># LDA</a> <a href="/tags/LinearClassifier/" rel="tag"># LinearClassifier</a> <a href="/tags/LinearRegressor/" rel="tag"># LinearRegressor</a> <a href="/tags/Estimator/" rel="tag"># Estimator</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/79f15ca6.html" rel="prev" title="贝叶斯分类器"><i class="fa fa-chevron-left"></i> 贝叶斯分类器</a></div><div class="post-nav-item"><a href="/archives/24da735a.html" rel="next" title="语音识别-GMM">语音识别-GMM <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-数据集说明"><span class="nav-text">1. 数据集说明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-线性回归数据集-Boston房价"><span class="nav-text">1.1 线性回归数据集-Boston房价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-二分类数据集-乳腺癌"><span class="nav-text">1.2 二分类数据集-乳腺癌</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-多分类数据集-鸢尾花"><span class="nav-text">1.3 多分类数据集-鸢尾花</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-线性回归"><span class="nav-text">2. 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-公式法"><span class="nav-text">2.1 公式法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-局部加权线性回归LWLR"><span class="nav-text">2.2 局部加权线性回归LWLR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-岭回归"><span class="nav-text">2.3 岭回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Estimator"><span class="nav-text">2.4 Estimator</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-二分类sigmoid"><span class="nav-text">3. 二分类sigmoid</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-简单梯度下降"><span class="nav-text">3.1 简单梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-随机梯度下降"><span class="nav-text">3.2 随机梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Estimator"><span class="nav-text">3.3 Estimator</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-多分类softmax"><span class="nav-text">4. 多分类softmax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-LDA分类-降维"><span class="nav-text">5. LDA分类&#x2F;降维</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Tao Zhou" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Tao Zhou</p><div class="site-description" itemprop="description">学习笔记</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">61</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">126</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Tao Zhou</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">897k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">13:36</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0</div><script>function leancloudSelector(url) {
    url = encodeURI(url);
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.getAttribute('id'));
      var title = visitors.getAttribute('data-flag-title');

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
              leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .catch(error => {
                console.error('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.getAttribute('id'));
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (let item of results) {
            let { url, time } = item;
            leancloudSelector(url).innerText = time;
          }
          for (let url of entries) {
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
            'X-LC-Key': 'xoukWFyqvFIXJ6DfxLLYtsTP',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
      appKey: 'xoukWFyqvFIXJ6DfxLLYtsTP',
      placeholder: "Just go go",
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: false,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html><!-- rebuild by neat -->