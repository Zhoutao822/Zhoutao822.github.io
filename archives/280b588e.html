<!-- build time:Sat Jan 18 2020 23:09:01 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://zhoutao822.coding.me").hostname,root:"/",scheme:"Pisces",version:"7.7.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!0,preload:!0},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="Super!!!!参考：西瓜书第6章 支持向量机1. 间隔与支持向量给定训练样本集$D &#x3D; { (\boldsymbol{x}_1, y_1), (\boldsymbol{x}_2, y_2),…,(\boldsymbol{x}_m, y_m) }, y_i \in { -1, +1 }$，显然这是一个二分类问题，支持向量机的设想是在训练集$D$的样本空间中找到一个划分超平面，将不同类别的样本分开"><meta property="og:type" content="article"><meta property="og:title" content="支持向量机"><meta property="og:url" content="http://zhoutao822.coding.me/archives/280b588e.html"><meta property="og:site_name" content="Tao"><meta property="og:description" content="Super!!!!参考：西瓜书第6章 支持向量机1. 间隔与支持向量给定训练样本集$D &#x3D; { (\boldsymbol{x}_1, y_1), (\boldsymbol{x}_2, y_2),…,(\boldsymbol{x}_m, y_m) }, y_i \in { -1, +1 }$，显然这是一个二分类问题，支持向量机的设想是在训练集$D$的样本空间中找到一个划分超平面，将不同类别的样本分开"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svm0.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svm1.jpg"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svm2.jpg"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svm3.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svm4.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svm5.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svr0.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/svr1.png"><meta property="article:published_time" content="2018-11-07T09:39:31.000Z"><meta property="article:modified_time" content="2020-01-18T14:40:57.588Z"><meta property="article:author" content="Tao Zhou"><meta property="article:tag" content="Theory"><meta property="article:tag" content="SVM"><meta property="article:tag" content="SVR"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://114.116.9.65:7777/images/2020/01/18/svm0.png"><link rel="canonical" href="http://zhoutao822.coding.me/archives/280b588e.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>支持向量机 | Tao</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Tao" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tao</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://zhoutao822.coding.me/archives/280b588e.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Tao Zhou"><meta itemprop="description" content="学习笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tao"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">支持向量机</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-11-07 17:39:31" itemprop="dateCreated datePublished" datetime="2018-11-07T17:39:31+08:00">2018-11-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-18 22:40:57" itemprop="dateModified" datetime="2020-01-18T22:40:57+08:00">2020-01-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span> </a></span></span><span id="/archives/280b588e.html" class="post-meta-item leancloud_visitors" data-flag-title="支持向量机" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/archives/280b588e.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/archives/280b588e.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>15k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>14 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>Super!!!!</strong></p><p>参考：</p><blockquote><p>西瓜书第6章 支持向量机</p></blockquote><h2 id="1-间隔与支持向量"><a href="#1-间隔与支持向量" class="headerlink" title="1. 间隔与支持向量"></a>1. 间隔与支持向量</h2><p>给定训练样本集$D = { (\boldsymbol{x}_1, y_1), (\boldsymbol{x}_2, y_2),…,(\boldsymbol{x}_m, y_m) }, y_i \in { -1, +1 }$，显然这是一个二分类问题，支持向量机的设想是在训练集$D$的样本空间中找到一个划分超平面，将不同类别的样本分开。显然，在空间中这样的划分超平面有无穷多，我们需要寻找到最合适的那一个。</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/svm0.png" alt="svm0.png"></p><p>从直觉上我们直到，“正中间”的划分超平面应该会是很合适的，因为处于正中间的划分超平面距离两种分类的样本的距离是最远的，因此用作预测时鲁棒性很好，泛化能力强。如何定义这样的超平面呢？</p><a id="more"></a><p>在样本空间中，划分超平面可以定义为</p><p>$$<br>\boldsymbol{w}^T\boldsymbol{x} + b=0<br>$$</p><p>其中$\boldsymbol{w}=(w_1;w_2;…;w_d)$为法向量，决定了超平面的方向；$b$为位移项，决定超平面与原点之间的距离。那么样本空间中任意点$\boldsymbol{x}$到超平面$(\boldsymbol{w},b)$的距离为</p><p>$$<br>r = \frac{|\boldsymbol{w}^T\boldsymbol{x}+b|}{||\boldsymbol{w}||}<br>$$</p><p>假设超平面$(\boldsymbol{w}, b)$能将训练样本正确分类，即对于$(\boldsymbol{x}_i, y_i) \in D$，若$y_i = +1$，则有$\boldsymbol{w}^T\boldsymbol{x}_i +b &gt; 0$；若$y_i = -1$，则有$\boldsymbol{w}^T\boldsymbol{x}_i +b &lt; 0$。令</p><p>$$<br>\left{\begin{matrix}<br>\boldsymbol{w}^T\boldsymbol{x}_i + b \geqslant +1, \quad y_i = +1 \<br>\boldsymbol{w}^T\boldsymbol{x}_i + b \leqslant -1, \quad y_i = -1<br>\end{matrix}\right.<br>$$</p><p>这里有两个问题：</p><ol><li>为什么不等式可以映射到$\pm 1$，而且为什么不是其他数字；</li><li>为什么划分超平面必须是“正中间”，而不能对一种类别有偏好，即两个不等式右边的数字绝对值不等。</li></ol><ul><li>第一个问题的回答，由于左右两边可以同时缩放，因此可以缩放到为$\pm 1$的条件，这是为了后面计算更加方便；</li><li>均等分割也是为了后面公式计算更加方便，至于偏好，我想应该有其它方法基于SVM进行了拓展实现呃不均等分割。</li></ul><p><img src="http://114.116.9.65:7777/images/2020/01/18/svm1.jpg" alt="svm1.jpg"></p><p>如图所示，距离超平面最近的几个训练样本可以使等号成立，它们被称为支持向量，两个异类支持向量到超平面的距离之和为</p><p>$$<br>\gamma = \frac{2}{||\boldsymbol{w}||}<br>$$</p><p>$\gamma$被称为间隔，双竖线表示$\boldsymbol{w}$的各成分的平方和再开方，若里面是两个向量，物理意义表示两个向量的欧拉距离。</p><p>我们的目标是找到最大间隔，即</p><p>$$<br>\underset{\boldsymbol{w}, b}{\max} \frac{2}{||\boldsymbol{w}||}<br>\<br>s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geqslant 1, \quad i = 1,2,…,m<br>$$</p><p>最大化$||\boldsymbol{w}||^{-1}$等效于最小化$||\boldsymbol{w}||^2$，加平方是为了抵消开方，简化后面的计算，于是可以重写为</p><p>$$<br>\underset{\boldsymbol{w},b}{\min} \frac{1}{2} ||\boldsymbol{w}||^2<br>\<br>s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geqslant 1, \quad i = 1,2,…,m<br>$$</p><p>这就是支持向量机（SVM）的基本型。</p><h2 id="2-对偶问题"><a href="#2-对偶问题" class="headerlink" title="2. 对偶问题"></a>2. 对偶问题</h2><p>我们希望求解上式得到最大间隔划分超平面所对应的模型</p><p>$$<br>f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b<br>$$</p><p>这个问题可以使用拉格朗日乘子法求解，首先对约束条件添加拉格朗日乘子$\alpha_i \geqslant 0$</p><p>$$<br>L(\boldsymbol{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}||\boldsymbol{w}||^2 + \sum^m_{i=1} \alpha(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))<br>$$</p><p>其中$\boldsymbol{\alpha} = (\alpha_1;\alpha_2;…;\alpha_m)$，令$L(\boldsymbol{w},b,\boldsymbol{\alpha})$对$\boldsymbol{w}$和$b$的偏导数为0可得</p><p>$$<br>\boldsymbol{w} = \sum^m_{i=1} \alpha_iy_i\boldsymbol{x}<em>i<br>\<br>0=\sum^m</em>{i=1}\alpha_iy_i<br>$$</p><p>注意$||\boldsymbol{w}||^2 = \boldsymbol{w}^T\boldsymbol{w}$。</p><p>将上式代入到$L$中消去$\boldsymbol{w}$和$b$，我们就得到基本型的对偶问题</p><p>$$<br>\underset{\boldsymbol{\alpha}}{\max} \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\boldsymbol{x}<em>i^T\boldsymbol{x}_j<br>\<br>s.t. \quad \sum^m</em>{i=1}\alpha_iy_i = 0,<br>\<br>\alpha_i \geqslant 0, \quad i = 1,2,…,m<br>$$</p><p>对偶问题怎么得到的，我们可以看作对于$L$有三个参数$(\boldsymbol{w},b,\boldsymbol{\alpha})$，我们通过拉格朗日法求解出其中两个参数的表示形式$(\boldsymbol{w},b)$，那么$L$就只与$\boldsymbol{\alpha}$相关，与$\boldsymbol{\alpha}$相关的部分是$\sum^m_{i=1} \alpha(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))$，而由于约束条件可知$1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \leqslant 0$，所以系数$\alpha_i$越大，$L$越小，所以我们就得到了对偶问题。</p><p>如果我们求解出$\boldsymbol{\alpha}$后，再解出$(\boldsymbol{w},b)$，我们的模型就可以确定了</p><p>$$<br>f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b<br>\<br>= \sum^m_{i=1} \alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x} +b<br>$$</p><hr><p>下面我们研究如何求解出$\boldsymbol{\alpha}$，每一个拉格朗日乘子$\alpha_i$对应一个样本$(\boldsymbol{x}_i, y_i)$。由于基本型的约束条件，所以求解目标满足KKT条件</p><p>$$<br>\left{\begin{matrix}<br>\alpha_i \geqslant 0 \<br>y_if(\boldsymbol{x}_i)-1 \geqslant 0 \<br>\alpha_i(y_if(\boldsymbol{x}_i)-1) = 0<br>\end{matrix}\right.<br>$$</p><p>对KKT的简单认知为：不等式约束条件与拉格朗日乘子的乘积等于0，且这个条件决定了训练样本的分布位置，若$\alpha_i &gt; 0$，则必有$y_if(\boldsymbol{x}_i)=1$，这样的点是支持向量；若$\alpha_i = 0$，则该样本不会在模型表达式求和中出现，该点的位置属于正常分类，但是不在间隔边界上。</p><p><strong>SVM性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。</strong></p><p>基于上述约束条件，为了求解，使用SMO算法，其基本思路是先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值，由于存在约束条件$\sum^m_{i=1}\alpha_iy_i = 0$，一个参数可以由其他固定的参数导出，所以需要选择两个变量，固定其他的参数。</p><p>于是，SMO每次选择两个变量$\alpha_i$和$\alpha_j$，并固定其他参数。在参数初始化后，不断执行如下两个步骤直至收敛：</p><ul><li>选取一对需要更新的变量$\alpha_i$和$\alpha_j$；</li><li>固定$\alpha_i$和$\alpha_j$以外的参数，求解$\max \alpha_i + \alpha_j - \lambda \alpha_i \alpha_j$的方程获得更新后的$\alpha_i$和$\alpha_j$（这里忽略掉其他常数）。</li></ul><p>很容易发现，在参数更新过程中$\max$目标函数会越来越大，而且倘若选取的两个参数$(\alpha_i, \alpha_j)$中只要有一个不满足KKT条件，那么目标函数增幅会更大，而且KKT条件违背程度越大，增幅越大。于是我们可以考虑先选取违背KKT条件的参数$\alpha$进行更新，具体实现是选取两变量对应的样本之间的间隔最大。</p><p>我们选取$(\alpha_i, \alpha_j)$，约束条件重写为</p><p>$$<br>\alpha_iy_i + \alpha_jy_j = c, \quad \alpha_i \geqslant 0, \alpha_j \geqslant 0<br>$$</p><p>其中</p><p>$$<br>c = -\sum_{k \neq i,j}\alpha_ky_k<br>$$</p><p>而且$\max$可以化简为$\alpha_i + \alpha_j - \lambda \alpha_i \alpha_j$，这就是一个简单的二次规划问题，直接公式求解。</p><p>如何确定偏移项$b$，因为对任意支持向量$(\boldsymbol{x}_s, y_s)$有$y_sf(\boldsymbol{x}_s)=1$，即</p><p>$$<br>y_s(\sum_{i\in S}\alpha_iy_i\boldsymbol{x}^T_i\boldsymbol{x}_s +b) =1<br>$$</p><p>$S$为所有支持向量下标集，理论上$(\boldsymbol{x}_s, y_s)$可以取任意支持向量来求解$b$，这样会得到很多组$b$，实际上采用均值实现更鲁棒</p><p>$$<br>b= \frac{1}{|S|}\sum_{s\in S}(\frac{1}{y_s} - \sum_{i\in S}\alpha_iy_i\boldsymbol{x}^T_i\boldsymbol{x}_s)<br>$$</p><h2 id="3-核函数"><a href="#3-核函数" class="headerlink" title="3. 核函数"></a>3. 核函数</h2><p><img src="http://114.116.9.65:7777/images/2020/01/18/svm2.jpg" alt="svm2.jpg"></p><p>上面在求解SVM时，我们默认数据样本在其样本空间是可分的，但是实际上由于维度过高，我们并不清楚数据是否存在这样的超平面，这时，我们可以将数据从原始样本空间映射到一个更高维的特征空间（再生核希尔伯特空间），使得样本在这个特征空间内可分。研究表明，只要原始空间维度有限，那么必定存在一个更高维的空间是样本可分。</p><p>令$\phi(\boldsymbol{x})$表示映射后的特征向量，于是划分超平面模型为</p><p>$$<br>f(\boldsymbol{x}) = \boldsymbol{w}^T \phi(\boldsymbol{x}) + b<br>$$</p><p>同理修改约束条件和SVM基本型，那么对偶问题变成了</p><p>$$<br>\underset{\boldsymbol{\alpha}}{\max} \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\phi(\boldsymbol{x}<em>i)^T\phi(\boldsymbol{x}_j)<br>\<br>s.t. \quad \sum^m</em>{i=1}\alpha_iy_i = 0,<br>\<br>\alpha_i \geqslant 0, \quad i = 1,2,…,m<br>$$</p><p>但是由于直接计算高维$\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$，通常计算量大而且很困难，为了解决这个问题，设想一个函数：</p><p>$$<br>\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j) = \left \langle \phi(\boldsymbol{x}_i), \phi(\boldsymbol{x}_j) \right \rangle = \phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)<br>$$</p><p>也就是说我们使用原始空间的函数计算特征空间的内积，感觉很奇妙。重写上面各种式子，得到模型</p><p>$$<br>f(\boldsymbol{x}) = \boldsymbol{w}^T \phi(\boldsymbol{x}) + b<br>\<br>= \sum^m_{i=1}\alpha_iy_i\phi(\boldsymbol{x}<em>i)^T\phi(\boldsymbol{x}) + b<br>\<br>= \sum^m</em>{i=1}\alpha_iy_i\kappa(\boldsymbol{x},\boldsymbol{x}_i) + b<br>$$</p><p>这里$\kappa(\cdot, \cdot)$就是核函数。显然，若已知映射$\phi(\cdot)$的具体形式，那么可以写出核函数$\kappa(\cdot, \cdot)$，但是实际问题中我们很难知道高维映射$\phi(\cdot)$的具体形式，如何寻找合适的核函数呢？有定理：</p><p>$\kappa(\cdot, \cdot)$是定义在$\chi \times \chi$（$\chi$为输入空间）上的对称函数，则$\kappa$是核函数，且对于任意数据$D={ \boldsymbol{x}_1, \boldsymbol{x}_2,…,\boldsymbol{x}_m }$，核矩阵$\boldsymbol{K}$总是半正定：</p><p>$$<br>\boldsymbol{K}_{i,j} = \kappa(\boldsymbol{x}_i, \boldsymbol{x}_j)<br>$$</p><p><em>半正定：设$A$是$n$阶方阵，如果对任何非零向量$X$，都有$X^TAX \geqslant 0$，其中$X^T$表示$X$的转置，就称$A$为半正定矩阵。</em></p><p><strong>所以只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。</strong></p><p><img src="http://114.116.9.65:7777/images/2020/01/18/svm3.png" alt="svm3.png"></p><p>此外，函数组合也可以得到，若$\kappa_1,\kappa_2$为核函数，例如：</p><ul><li>对于任意正数$\gamma_1, \gamma_2$，线性组合：<br>$$<br>\gamma_1\kappa_1 + \gamma_2\kappa_2<br>$$</li><li>核函数直积：<br>$$<br>\kappa_1(\boldsymbol{x}, \boldsymbol{z})\kappa_2(\boldsymbol{x}, \boldsymbol{z})<br>$$</li><li>任意函数$g(\boldsymbol{x})$：<br>$$<br>g(\boldsymbol{x})\kappa_1(\boldsymbol{x}, \boldsymbol{z})g(\boldsymbol{z})<br>$$<br>都是核函数。</li></ul><h2 id="4-软间隔与正则化"><a href="#4-软间隔与正则化" class="headerlink" title="4. 软间隔与正则化"></a>4. 软间隔与正则化</h2><p>在上面的讨论中，我们始终假设划分超平面是存在的，实际上往往很难确定一个核函数使得样本线性可分，在此基础上我们可以适当放宽间隔的条件，允许一定的样本出错，为此，引入软间隔概念。</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/svm4.png" alt="svm4.png"></p><p>因此某些样本不满足约束</p><p>$$<br>y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \geqslant 1<br>$$</p><p>那么优化目标修正为</p><p>$$<br>\underset{\boldsymbol{w},b}{\min} \frac{1}{2} ||\boldsymbol{w}||^2 + C \sum^m_{i=1}l_{0/1}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1)<br>$$</p><p>其中$C &gt; 0$是一个常数，通过$C$的取值大小控制对错误样本的容忍度，$C$越大，对错误样本的容忍度越小，对约束条件越严格，当$C$取有限值，允许一些样本不满足约束。$l_{0/1}$是“0/1损失函数”</p><p>$$<br>l_{0/1}(z) = \left{\begin{matrix}<br>1, \quad if \quad z &lt; 0 \<br>0, \quad otherwise<br>\end{matrix}\right.<br>$$</p><p>然而$l_{0/1}$非凸、非连续，数学性质不太好，实际中常用其他函数替代，称为替代损失函数，它们通常是凸的连续函数且是$l_{0/1}$的上界。三种常用的替代损失函数：</p><ol><li>hinge损失：$l_{hinge}(z) = max(0, 1-z)$</li><li>指数损失：$l_{exp}(z) = exp(-z)$</li><li>对率损失：$l_{log}(z) = log(1+exp(-z))$</li></ol><p><img src="http://114.116.9.65:7777/images/2020/01/18/svm5.png" alt="svm5.png"></p><p>若要替换，则$z = y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)$。若引入松弛变量$\xi_i \geqslant 0$，则优化目标重写为</p><p>$$<br>\underset{\boldsymbol{w},b,\xi_i}{\min} \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}\xi_i<br>\<br>s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \geqslant 1 - \xi_i<br>\<br>\xi_i \geqslant 0, \quad i = 1,2,…,m<br>$$</p><p>这就是常用的软间隔支持向量机。</p><p>显然，每一个样本对应一个松弛变量，用以表征该样本不满足约束的程度。于是同样拉格朗日法得到</p><p>$$<br>L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi}, \boldsymbol{\mu}) = \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}\xi_i<br>\</p><ul><li>\sum^m_{i=1}\alpha_i(1-\xi_i -y_i(\boldsymbol{w}^T\boldsymbol{x}<em>i + b)) - \sum^m</em>{i=1}\mu_i\xi_i<br>$$</li></ul><p>其中拉格朗日乘子$\alpha_i \geqslant 0, \mu_i \geqslant 0$。</p><p>令$L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi}, \boldsymbol{\mu})$对$(\boldsymbol{w}, b,\xi_i)$偏导数为0可得</p><p>$$<br>\boldsymbol{w} = \sum^m_{i=1}\alpha_iy_i\boldsymbol{x}<em>i<br>\<br>0 = \sum^m</em>{i=1}\alpha_iy_i<br>\<br>C = \alpha_i + \mu_i<br>$$</p><p>同样得到对偶问题</p><p>$$<br>\underset{\boldsymbol{\alpha}}{\max} \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\boldsymbol{x}<em>i^T\boldsymbol{x}_j<br>\<br>s.t. \quad \sum^m</em>{i=1}\alpha_iy_i = 0,<br>\<br>0 \leqslant \alpha_i \leqslant C, \quad i = 1,2,…,m<br>$$</p><p>显然，这与最开始的对偶问题唯一的区别是$\alpha_i$的范围。</p><p>类似的KKT条件为</p><p>$$<br>\left{\begin{matrix}<br>\alpha_i \geqslant 0 , \quad \mu_i \geqslant 0\<br>y_if(\boldsymbol{x}_i)-1+ \xi_i \geqslant 0 \<br>\alpha_i(y_if(\boldsymbol{x}_i)-1+ \xi_i) = 0 \<br>\xi_i \geqslant 0, \quad \mu_i\xi_i = 0<br>\end{matrix}\right.<br>$$</p><p>对任意训练样本$(\boldsymbol{x}_i, y_i)$，总有$\alpha_i = 0$或$y_if(\boldsymbol{x}_i) = 1 - \xi_i$，而且：</p><ul><li>若$\alpha_i=0$，则该样本不会对$f(\boldsymbol{x})$有任何影响；</li><li>若$\alpha_i &gt; 0$，则必有$y_if(\boldsymbol{x}_i) = 1 - \xi_i$，即支持向量；</li><li>若$\alpha_i &lt; C$，则$\mu_i &gt; 0$，进而有$\xi_i=0$，即该样本恰在最大间隔边界上；</li><li>若$\alpha_i = C$，则有$\mu_i = 0$，此时若$\xi_i \leqslant 1$则该样本落在最大间隔内部，若$\xi_i &gt; 1$则样本被错误分类。</li></ul><p>由此看出，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持了稀疏性。</p><p>使用对率损失函数$l_{log}$替换，几乎得到了对率回归模型。</p><p>将优化目标写为一般形式</p><p>$$<br>\underset{f}{\min} \Omega(f) + C \sum^m_{i=1}l(f(\boldsymbol{x}_i), y_i)<br>$$</p><p>其中$\Omega(f)$称为结构风险，用于描述模型$f$的某些性质；第二项$\sum^m_{i=1}l(f(\boldsymbol{x}_i), y_i)$称为经验风险，用于描述模型与训练数据的契合程度；$C$对二者进行折中。从正则化角度看，$\Omega(f)$称为正则化项，$C$称为正则化常数。也就意味着可以使用$L_p$范数进行调整，$L_2$范数倾向于$\boldsymbol{w}$的分量取值尽量均衡，即非零分量个数尽量稠密，而$L_0$或$L_1$范数则倾向与$\boldsymbol{w}$的分量尽量稀疏，即非零分量个数尽量少。</p><h2 id="5-支持向量回归"><a href="#5-支持向量回归" class="headerlink" title="5. 支持向量回归"></a>5. 支持向量回归</h2><p>支持向量机用于回归问题，希望学得的模型$f(\boldsymbol{x})$与$y$尽可能接近。</p><p>传统回归模型直接基于模型输出$f(\boldsymbol{x})$与真实输出$y$之间的差别来计算损失，当且仅当两者完全相同时，损失才为0.与此不同，支持向量回归SVR允许我们容忍$f(\boldsymbol{x})$与$y$之间最多有$\epsilon$的偏差，仅当两者之间的差别绝对值大于$\epsilon$时才计算损失。这相当于以$f(\boldsymbol{x})$为中心构建了一个宽度为$2\epsilon$的间隔带，只有落入间隔带的样本被认为是预测正确的。</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/svr0.png" alt="svr0.png"></p><p>于是SVR问题可形式化为</p><p>$$<br>\underset{\boldsymbol{w},b}{\min} \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}l_{\epsilon}(f(\boldsymbol{x}_i)-y_i)<br>$$</p><p>其中$C$为正则化常数，$l_\epsilon$为$\epsilon$-不敏感损失函数</p><p>$$<br>l_\epsilon(z) = \left{\begin{matrix}<br>0, \quad if |z| \leqslant \epsilon \<br>|z| -\epsilon , \quad otherwise<br>\end{matrix}\right.<br>$$</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/svr1.png" alt="svr1.png"></p><p>引入松弛变量$\xi_i$和$\hat{\xi}_i$，重写上式为</p><p>$$<br>\underset{\boldsymbol{w},b,\xi_i, \hat{\xi}<em>i}{\min} \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m</em>{i=1}(\xi_i + \hat{\xi}_i)<br>\<br>s.t. \quad f(\boldsymbol{x}_i) - y_i \leqslant \epsilon + \xi_i<br>\<br>y_i - f(\boldsymbol{x}_i) \leqslant \epsilon + \hat{\xi}_i<br>\<br>\xi_i \geqslant 0, \hat{\xi}_i \geqslant 0, \quad i = 1,2,…,m<br>$$</p><p>同理引入拉格朗日乘子$(\mu_i, \hat{\mu}_i, \alpha_i, \hat{\alpha}_i) \geqslant 0$</p><p>$$<br>L(\boldsymbol{w},b,\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}},\boldsymbol{\mu}, \hat{\boldsymbol{\mu}})<br>\<br>= \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}(\xi_i+\hat{\xi}<em>i) - \sum^m</em>{i=1}\mu_i\xi_i - \sum^m_{i=1}\hat{\mu}_i\hat{\xi}_i<br>\</p><ul><li>\sum^m_{i=1}\alpha_i(f(\boldsymbol{x}<em>i) -y_i -\epsilon -\xi_i) + \sum^m</em>{i=1}\hat{\alpha}_i(y_i - f(\boldsymbol{x}_i)-\epsilon -\hat{\xi}_i)<br>$$</li></ul><p>令$L(\boldsymbol{w},b,\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}},\boldsymbol{\mu}, \hat{\boldsymbol{\mu}})$对$\boldsymbol{w},b,\xi_i,\hat{\xi}_i$偏导数为0</p><p>$$<br>\boldsymbol{w} = \sum^m_{i=1}(\hat{\alpha}<em>i - \alpha_i)\boldsymbol{x}_i<br>\<br>0 = \sum^m</em>{i=1}(\hat{\alpha}_i - \alpha_i)<br>\<br>C = \alpha_i + \mu_i<br>\<br>C = \hat{\alpha}_i + \hat{\alpha}_i<br>$$</p><p>代入，得到SVR对偶问题</p><p>$$<br>\underset{\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}}{\max} \sum^m_{i=1}y_i(\hat{\alpha}_i - \alpha_i) - \epsilon(\hat{\alpha}_i + \alpha_i)<br>\</p><ul><li>\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}(\hat{\alpha}<em>i - \alpha_i)(\hat{\alpha}_j - \alpha_j)\boldsymbol{x}_i^T\boldsymbol{x}_j<br>\<br>s.t. \quad \sum^m</em>{i=1}(\hat{\alpha}_i - \alpha_i) = 0<br>\<br>0 \leqslant \alpha_i, \hat{\alpha}_i \leqslant C<br>$$</li></ul><p>上述过程需满足KKT条件，即</p><p>$$<br>\left{\begin{matrix}<br>\alpha_i(f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i) = 0 \<br>\hat{\alpha}_i(y_i - f(\boldsymbol{x}_i)-\epsilon-\xi_i) = 0 \<br>\alpha_i\hat{\alpha}_i = 0, \xi_i\hat{\xi}_i = 0\<br>(C - \alpha_i) \xi_i = 0,(C-\hat{\alpha}_i)\hat{\xi}_i = 0<br>\end{matrix}\right.<br>$$</p><p>由上式可知，当且仅当样本$(\boldsymbol{x}_i,y_i)$不落入$\epsilon$-间隔带中，相应的$\alpha_i$和$\hat{\alpha}_i$才能取非零值。此外约束$f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i = 0$和$y_i - f(\boldsymbol{x}_i)-\epsilon-\xi_i = 0$不能同时成立，因此$\alpha_i$和$\hat{\alpha}_i$中至少有一个为0。</p><p>同理，SVR的解形如</p><p>$$<br>f(\boldsymbol{x}) = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\boldsymbol{x}_i^T\boldsymbol{x} + b<br>$$</p><p>其中使$(\hat{\alpha}_i - \alpha_i) \neq 0$的样本即为SVR的支持向量，它们必定落在间隔带之外。显然，也是稀疏解。</p><p>由KKT条件可知，对每个样本$(\boldsymbol{x}_i,y_i)$都有$(C - \alpha_i) \xi_i = 0$且$\alpha_i(f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i) = 0$。于是，在得到$\alpha_i$后，若$0 &lt; \alpha_i &lt; C$，则必有$\xi_i = 0$，进而有</p><p>$$<br>b = y_i + \epsilon - \sum^m_{j=1}(\hat{\alpha}_j - \alpha_j)\boldsymbol{x}_j^T\boldsymbol{x}_i<br>$$</p><p>理论上对于任意满足$0&lt;\alpha_i&lt;C$的样本都可以求得$b$。也就是多组解，实践中通常选取多个或所有满足的样本求解平均值，参考SVM。</p><p>若考虑特征映射，则</p><p>$$<br>\boldsymbol{w} = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\phi(\boldsymbol{x}_i)<br>$$</p><p>同理，带核函数的SVR表示形式改变为</p><p>$$<br>f(\boldsymbol{x}) = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\kappa(\boldsymbol{x}, \boldsymbol{x}_i) + b<br>$$</p><h2 id="6-核方法"><a href="#6-核方法" class="headerlink" title="6. 核方法"></a>6. 核方法</h2><p>观察SVM和SVR的模型表示形式（考虑核函数），若不考虑偏移项$b$，最终模型总能表达成核函数的线性组合。一个更一般的结论称为表示定理：</p><blockquote><p><strong>表示定理</strong> 令$\mathbb{H}$为核函数$\kappa$对应的再生核希尔伯特空间，$||h||_\mathbb{H}$表示$\mathbb{H}$空间中关于$h$的范数，对于任意单调递增函数$\Omega:[0, \infty] \rightarrow \mathbb{R}$和任意非负损失函数$l:\mathbb{R}^m \rightarrow [0, \infty]$，优化问题</p></blockquote><p>$$<br>\underset{h\in \mathbb{H}}{\min} F(h) = \Omega(||h||_\mathbb{H}) + l(h(\boldsymbol{x}_1), h(\boldsymbol{x}_2),…,h(\boldsymbol{x}_m))<br>$$</p><p>的解总可写为</p><p>$$<br>h^*(\boldsymbol{x}) = \sum^m_{i=1}\alpha_i \kappa(\boldsymbol{x}, \boldsymbol{x}_i)<br>$$</p><p>表示定理对损失函数没有限制，对正则化项$\Omega$仅要求单调递增，甚至不要求$\Omega$为凸函数，这意味着对于一般的损失函数和正则化项，优化问题的最优解都可以表示为核函数的线性组合。</p><hr><p>下面介绍通过引入核函数拓展<a href="http://zhoutao822.coding.me/2018/11/07/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">线性判别分析LDA</a>为非线性学习器，从而得到核线性判别分析KLDA。</p><p>先假设有映射$\phi:\chi \rightarrow \mathbb{F}$将样本映射到一个特征空间，然后在$\mathbb{F}$中执行线性判别分析，以求得</p><p>$$<br>h(\boldsymbol{x}) = \boldsymbol{w}^T\phi(\boldsymbol{x})<br>$$</p><p>类似LDA的类内散度矩阵$\boldsymbol{S}^\phi_b$和类间散度矩阵$\boldsymbol{S}^\phi_w$，KLDA的学习目标是</p><p>$$<br>\underset{\boldsymbol{w}}{\max} J(\boldsymbol{w}) = \frac{\boldsymbol{w}^T\boldsymbol{S}^{\phi}_b\boldsymbol{w}}{\boldsymbol{w}^T\boldsymbol{S}^{\phi}_w\boldsymbol{w}}<br>$$</p><p>令$X_i$表示第$i \in { 0,1 }$类样本的集合，其样本数为$m_i$，样本总数为$m = m_0+m_1$。第$i$类样本在特征空间$\mathbb{F}$中的均值为</p><p>$$<br>\boldsymbol{\mu}^\phi_i = \frac{1}{m_i}\sum_{\boldsymbol{x}\in X_i}\phi(\boldsymbol{x})<br>$$</p><p>两个散度矩阵分别为</p><p>$$<br>\boldsymbol{S}<em>b^\phi = (\boldsymbol{\mu}^\phi_1 - \boldsymbol{\mu}^\phi_0)(\boldsymbol{\mu}^\phi_1 - \boldsymbol{\mu}^\phi_0)^T<br>\<br>\boldsymbol{S}_w^\phi = \sum^1</em>{i=0}\sum_{\boldsymbol{x}\in X_i}(\phi(\boldsymbol{x})-\boldsymbol{\mu}^\phi_i)(\phi(\boldsymbol{x})-\boldsymbol{\mu}^\phi_i)^T<br>$$</p><p>同理使用核函数$\kappa(\boldsymbol{x}, \boldsymbol{x}_i) = \phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x})$隐式表达映射$\phi$和特征空间$\mathbb{F}$。把$J(\boldsymbol{w})$作为损失函数$l$，再令$\Omega \equiv 0$，由表示定理，函数$h(\boldsymbol{x})$可写为</p><p>$$<br>h(\boldsymbol{x}) = \sum^m_{i=1}\alpha_i \kappa(\boldsymbol{x}, \boldsymbol{x}<em>i)<br>\<br>\boldsymbol{w} = \sum^m</em>{i=1}\alpha_i\phi(\boldsymbol{x}_i)<br>$$</p><p>令$\boldsymbol{K} \in \mathbb{R}^{m \times m}$为核函数$\kappa$对应的核矩阵，$(\boldsymbol{K})_{ij} = \kappa(\boldsymbol{x}_i, \boldsymbol{x}_j)$。令$\boldsymbol{1}_i \in { 1,0 }^{m \times 1}$为第$i$类样本的指示向量，即$\boldsymbol{1}_i$的第$j$个分量为1当且仅当$\boldsymbol{x}_j \in X_i$，否则第$j$个分量为0。再令</p><p>$$<br>\boldsymbol{\hat{\mu}}<em>0 = \frac{1}{m_0}\boldsymbol{K}\boldsymbol{1}_0<br>\<br>\boldsymbol{\hat{\mu}}_1 = \frac{1}{m_1}\boldsymbol{K}\boldsymbol{1}_1<br>\<br>\boldsymbol{M} = (\boldsymbol{\hat{\mu}}_0 - \boldsymbol{\hat{\mu}}_1)(\boldsymbol{\hat{\mu}}_0 - \boldsymbol{\hat{\mu}}_1)^T<br>\<br>\boldsymbol{N} = \boldsymbol{K}\boldsymbol{K}^T - \sum^1</em>{i=0}m_i\boldsymbol{\hat{\mu}}_i\boldsymbol{\hat{\mu}}_i^T<br>$$</p><p>于是$\max$等价为</p><p>$$<br>\underset{\boldsymbol{\alpha}}{\max} J(\boldsymbol{\alpha}) = \frac{\boldsymbol{\alpha}^T\boldsymbol{M}\boldsymbol{\alpha}}{\boldsymbol{\alpha}^T\boldsymbol{N}\boldsymbol{\alpha}}<br>$$</p><p>根据上式再是同线性判别分析求解方法即可得到$\boldsymbol{\alpha}$，进而得到投影函数$h(\boldsymbol{x})$，具体参考<a href="http://zhoutao822.coding.me/2018/11/07/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><code>线性模型</code></a>。</p></div><div class="popular-posts-header">推荐文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/7d1dcda7.html" rel="bookmark">模型评估与选择</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/8ddc7426.html" rel="bookmark">决策树</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/7ca31f7.html" rel="bookmark">神经网络</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/fcb2659b.html" rel="bookmark">数据集</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/5206f73f.html" rel="bookmark">线性模型</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tao Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://zhoutao822.coding.me/archives/280b588e.html" title="支持向量机">http://zhoutao822.coding.me/archives/280b588e.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Theory/" rel="tag"># Theory</a> <a href="/tags/SVM/" rel="tag"># SVM</a> <a href="/tags/SVR/" rel="tag"># SVR</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/7ca31f7.html" rel="prev" title="神经网络"><i class="fa fa-chevron-left"></i> 神经网络</a></div><div class="post-nav-item"><a href="/archives/79f15ca6.html" rel="next" title="贝叶斯分类器">贝叶斯分类器 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-间隔与支持向量"><span class="nav-text">1. 间隔与支持向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-对偶问题"><span class="nav-text">2. 对偶问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-核函数"><span class="nav-text">3. 核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-软间隔与正则化"><span class="nav-text">4. 软间隔与正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-支持向量回归"><span class="nav-text">5. 支持向量回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-核方法"><span class="nav-text">6. 核方法</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Tao Zhou" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Tao Zhou</p><div class="site-description" itemprop="description">学习笔记</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">61</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">126</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Tao Zhou</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">897k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">13:36</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0</div><script>function leancloudSelector(url) {
    url = encodeURI(url);
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.getAttribute('id'));
      var title = visitors.getAttribute('data-flag-title');

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
              leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .catch(error => {
                console.error('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.getAttribute('id'));
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (let item of results) {
            let { url, time } = item;
            leancloudSelector(url).innerText = time;
          }
          for (let url of entries) {
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
            'X-LC-Key': 'xoukWFyqvFIXJ6DfxLLYtsTP',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
      appKey: 'xoukWFyqvFIXJ6DfxLLYtsTP',
      placeholder: "Just go go",
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: false,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html><!-- rebuild by neat -->