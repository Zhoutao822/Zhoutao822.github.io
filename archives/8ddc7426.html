<!-- build time:Sat Jan 18 2020 23:09:01 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://zhoutao822.coding.me").hostname,root:"/",scheme:"Pisces",version:"7.7.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!0,preload:!0},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="树即是人，人即是树参考：西瓜书第4章 决策树决策树系列（一）——基础知识回顾与总结1. 基本流程在日常生活中其实我们都可能在使用决策树算法，只是你没有这个概念，比如说，在这个看脸的时代，你在妹子心中的状态可以用下图描述这样就完成了一个三分类决策树，每一个判断节点都是一个属性，每个节点的分支都是该属性的属性值。"><meta property="og:type" content="article"><meta property="og:title" content="决策树"><meta property="og:url" content="http://zhoutao822.coding.me/archives/8ddc7426.html"><meta property="og:site_name" content="Tao"><meta property="og:description" content="树即是人，人即是树参考：西瓜书第4章 决策树决策树系列（一）——基础知识回顾与总结1. 基本流程在日常生活中其实我们都可能在使用决策树算法，只是你没有这个概念，比如说，在这个看脸的时代，你在妹子心中的状态可以用下图描述这样就完成了一个三分类决策树，每一个判断节点都是一个属性，每个节点的分支都是该属性的属性值。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/tree.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/precut.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/backcut.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/tree1.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/tree3.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/tree2.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/0.png"><meta property="og:image" content="http://114.116.9.65:7777/images/2020/01/18/1.png"><meta property="article:published_time" content="2018-11-07T08:32:20.000Z"><meta property="article:modified_time" content="2020-01-18T14:38:33.709Z"><meta property="article:author" content="Tao Zhou"><meta property="article:tag" content="Theory"><meta property="article:tag" content="Decision Tree"><meta property="article:tag" content="Regression Decision Tree"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://114.116.9.65:7777/images/2020/01/18/tree.png"><link rel="canonical" href="http://zhoutao822.coding.me/archives/8ddc7426.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>决策树 | Tao</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Tao" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tao</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://zhoutao822.coding.me/archives/8ddc7426.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Tao Zhou"><meta itemprop="description" content="学习笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tao"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">决策树</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-11-07 16:32:20" itemprop="dateCreated datePublished" datetime="2018-11-07T16:32:20+08:00">2018-11-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-18 22:38:33" itemprop="dateModified" datetime="2020-01-18T22:38:33+08:00">2020-01-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span> </a></span></span><span id="/archives/8ddc7426.html" class="post-meta-item leancloud_visitors" data-flag-title="决策树" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/archives/8ddc7426.html#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/archives/8ddc7426.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.4k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><strong>树即是人，人即是树</strong></p><p>参考：</p><blockquote><p>西瓜书第4章 决策树<br><a href="http://www.cnblogs.com/yonghao/p/5061873.html" target="_blank" rel="noopener">决策树系列（一）——基础知识回顾与总结</a></p></blockquote><h2 id="1-基本流程"><a href="#1-基本流程" class="headerlink" title="1. 基本流程"></a>1. 基本流程</h2><p>在日常生活中其实我们都可能在使用决策树算法，只是你没有这个概念，比如说，在这个看脸的时代，你在妹子心中的状态可以用下图描述</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/tree.png" alt="tree.png"></p><p>这样就完成了一个三分类决策树，每一个判断节点都是一个属性，每个节点的分支都是该属性的属性值。</p><a id="more"></a><hr><p><strong>决策树学习基本算法</strong></p><p>输入：训练集$D = { (\boldsymbol{x}_1, y_1), (\boldsymbol{x}_2, y_2), …, (\boldsymbol{x}_m, y_m) }$；</p><p>过程：函数$TreeGenerate(D, A)$</p><p>01：生成结点$node$；</p><p>02：$if$ $D$中样本全属于同一类别$C$ $then$</p><p>03：&emsp;将$node$标记为$C$类叶结点；$return$</p><p>04：$end$ $if$</p><p>05：$if$ $A = \emptyset$ $OR$ $D$ 中样本在$A$上的取值相同 $then$</p><p>06：&emsp;将$node$标记为叶结点，其类别标记为$D$中样本数最多的类；$return$</p><p>07：$end$ $if$</p><p>08：从$A$中选择最优化分属性$a_k$；</p><p>09：$for$ $a_k$的每一个值$a^v_k$ $do$</p><p>10：&emsp;为$node$生成一个分支；令$D_v$表示$D$中在$a_k$上取值为$a_k^v$的样本子集；</p><p>11：&emsp;$if$ $D_v$为空 $then$</p><p>12：&emsp;&emsp;将分支结点标记为叶结点，其标记类别为$D$中样本最多的类；$return$</p><p>13：&emsp;$else$</p><p>14：&emsp;&emsp;以$TreeGenerate(D_v, A \setminus {a_k })$为分支结点</p><p>15：&emsp;$end$ $if$</p><p>16：$end$ $for$</p><p>输出：以$node$为根结点的一棵决策树</p><hr><p>递归生成决策树有三种情形会导致递归返回：</p><ol><li>当前结点包含的样本全属于同一类别，无需划分；</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；</li><li>当前结点包含的样本集合为空，不能划分。</li></ol><h2 id="2-划分选择"><a href="#2-划分选择" class="headerlink" title="2. 划分选择"></a>2. 划分选择</h2><p>决策树学习的关键在第8行，即如何选择最优划分属性。一般来说，我们希望分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”也来越高（如何衡量“纯度”，有不同的算法）。</p><h3 id="2-1-信息增益"><a href="#2-1-信息增益" class="headerlink" title="2.1 信息增益"></a>2.1 信息增益</h3><p>假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k = 1,2,…,|\mathbb{Y}|)$，则$D$的信息熵定义为</p><p>$$<br>Ent(D) = -\sum^{|\mathbb{Y}|}_{k=1}p_k\log_2p_k<br>$$</p><p>$Ent(D)$的值越小，则$D$的纯度越高。</p><p>假定离散属性$a$有$V$个可能的取值${ a^1, a^2, …,a^V }$，若使用$a$来对样本集合$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。则可以计算用属性$a$对$D$进行划分所获得的信息增益</p><p>$$<br>Gain(D, a) = Ent(D) - \sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)<br>$$</p><p>一般而言，信息增益越大，则使用属性$a$来进行划分所获得的纯度提升越大。因此，在第8行使用属性$a_k = \underset{a \in A}{\arg \max Gain(D, a)}$。这就是$ID3$决策树学习算法的划分准则。</p><h3 id="2-2-增益率"><a href="#2-2-增益率" class="headerlink" title="2.2 增益率"></a>2.2 增益率</h3><p>利用信息增益作为划分准则可能会带来另一问题，即对可取值数目较多的属性有所偏好，比如说若一个属性有10个属性值，而另一个属性只有3个属性值，此时经过计算很大程度上10个属性值的属性带来的信息增益会大于另一个。为减少这种偏好可能带来的不利影响，$C4.5$决策树算法使用增益率作为划分准则</p><p>$$<br>Gain_ratio(D, a) = \frac{Gain(D,a)}{IV(a)}<br>\<br>IV(a) = -\sum^V_{v=1}\frac{|D^v|}{|D|} \log_2\frac{|D^v|}{|D|}<br>$$</p><p>$IV(a)$称为属性$a$的固有值，若属性$a$的取值数目越多，则$IV(a)$的值通常会越大。</p><p>与此同时$C4.5$算法具体流程并不是直接选择增益率最高的属性，而是先从候选属性中选择信息增益高于平均水平的属性，再从中选择增益率高的。</p><h4 id="2-3-基尼指数"><a href="#2-3-基尼指数" class="headerlink" title="2.3 基尼指数"></a>2.3 基尼指数</h4><p>$CART$决策树（可用于分类和回归）使用基尼指数选择划分属性</p><p>$$<br>Gini(D) = \sum^{|\mathbb{Y}|}<em>{k=1}\sum</em>{k’ \neq k}p_kp_{k’}<br>\<br>= 1- \sum^{|\mathbb{Y}|}_{k=1}p_k^2<br>$$</p><p>$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$越小，则数据集$D$纯度越高。</p><p>则对应属性$a$的基尼指数定义为</p><p>$$<br>Gini_index(D, a) = \sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)<br>$$</p><p>我们选择使得划分后基尼指数最小的属性作为最优属性，即</p><p>$$<br>a_k = \underset{a \in A}{\arg \min}Gini_index(D, a)<br>$$</p><h2 id="3-剪枝处理"><a href="#3-剪枝处理" class="headerlink" title="3. 剪枝处理"></a>3. 剪枝处理</h2><p>剪枝是对付过拟合的主要手段，主要方式是删除一些分支，基本策略有两种，预剪枝和后剪枝。</p><ul><li>预剪枝是在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分当前结点并标记为叶结点；</li><li>后剪枝是先从数据集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将子树替换为叶结点。</li></ul><p>如何判断泛化性能，我们使用验证集进行评估。</p><h3 id="3-1-预剪枝"><a href="#3-1-预剪枝" class="headerlink" title="3.1 预剪枝"></a>3.1 预剪枝</h3><p><img src="http://114.116.9.65:7777/images/2020/01/18/precut.png" alt="precut.png"></p><ol><li>根据划分准则选择最优划分属性；</li><li>计算不对当前最优属性划分的条件下验证集的准确率；</li><li>计算对当前最优属性划分后验证集的准确率；</li><li>比较上面两步的值，若划分后的准确率不大于划分前，则放弃使用该属性划分。</li></ol><p>优劣：</p><ul><li>预剪枝是基于贪心算法的，必然存在继续划分可能带来泛化性能提升的可能性，使用预剪枝导致永远无法展开这些分支；</li><li>预剪枝降低过拟合的风险，减少了训练开销。</li></ul><h3 id="3-2-后剪枝"><a href="#3-2-后剪枝" class="headerlink" title="3.2 后剪枝"></a>3.2 后剪枝</h3><p><img src="http://114.116.9.65:7777/images/2020/01/18/backcut.png" alt="backcut.png"></p><ol><li>根据训练集生成完整的决策树；</li><li>计算当前决策树在验证集上的准确率；</li><li>自底向上考虑某个属性，计算将当前属性替换为叶结点（取最多的类别作为叶结点标记）后的验证集准确率；</li><li>比较上面两步的值，若剪枝后的准确率不大于剪枝前的，则放弃使用该属性划分。</li></ol><p>优劣：</p><ul><li>后剪枝保留的分支较预剪枝多，所以欠拟合风险小，泛化性能优于预剪枝；</li><li>自底向上的过程导致训练开销远大于预剪枝。</li></ul><h2 id="4-连续与缺失值"><a href="#4-连续与缺失值" class="headerlink" title="4. 连续与缺失值"></a>4. 连续与缺失值</h2><h3 id="4-1-连续值处理"><a href="#4-1-连续值处理" class="headerlink" title="4.1 连续值处理"></a>4.1 连续值处理</h3><p>对于连续值，最简单的策略是采用二分法，即将属性$a$的属性值基于$t$划分为两个阵营$D_t^-$和$D_t^+$，如何确定$t$的值才能使该划分为最优化分，显然，$t$的取值在$[a^i, a^{i+1})$之间都是等效的，那么我们可以参照插队的方式取$t$的所有可能值，再从中选取最优点。</p><p>对于有$n$个属性值的连续属性$a$，$t$的候选划分点集和</p><p>$$<br>T_a = { \frac{a^i + a^{i+1}}{2}|1 \leqslant i \leqslant n-1 }<br>$$</p><p>通过对信息增益表达式的改造</p><p>$$<br>Gain(D, a) = \underset{t \in T_a}{\max} Gain(D, a, t)<br>\<br>= \underset{t \in T_a}{\max} Ent(D) - \sum_{\lambda \in { -,+ }}\frac{|D^{\lambda}_t|}{|D|}Ent(D_t^{\lambda})<br>$$</p><p>需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。</p><h3 id="4-2-缺失值处理"><a href="#4-2-缺失值处理" class="headerlink" title="4.2 缺失值处理"></a>4.2 缺失值处理</h3><p>样本不完整，那么该样本应该怎样划分呢。</p><p>问题等价于</p><ol><li>如何在属性缺失的情况下进行划分属性选择？</li><li>给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</li></ol><p>比较简单的方式是根据数据集$D$中在属性$a$上没有缺失值的样本来对属性$a$进行划分。</p><hr><p>对问题（1），令$\tilde{D}$表示$D$在属性$a$上没有缺失值的样本子集，<br>假定属性$a$有$V$个可取值${ a^1, a^2,…,a^V }$，<br>令$\tilde{D}^v$表示$\tilde{D}$中在属性$a$上取值为$a^v$的样本子集，<br>$\tilde{D}<em>k$表示$\tilde{D}$中属于第$k$类（$k = 1, 2,…,|\mathbb{Y}|$）的样本子集，则显然有$\tilde{D} = \bigcup</em>{k=1}^{|\mathbb{Y}|}\tilde{D}<em>k = \bigcup</em>{v=1}^{|V|}\tilde{D}^v$。<br>假定为每个样本$\boldsymbol{x}$初始化一个权重$w_{\boldsymbol{x}}$（初始值全为1），并定义</p><p>$$<br>\rho = \frac{\sum_{\boldsymbol{x}\in \tilde{D}}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in D}w_{\boldsymbol{x}}}<br>\<br>\tilde{p}<em>k = \frac{\sum</em>{\boldsymbol{x}\in \tilde{D}<em>k}w</em>{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in \tilde{D}}w_{\boldsymbol{x}}}<br>\<br>\tilde{r}<em>v = \frac{\sum</em>{\boldsymbol{x}\in \tilde{D}^v}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in \tilde{D}}w_{\boldsymbol{x}}}<br>$$</p><p>对属性$a$来说，$\rho$表示无缺失值样本所占的比例，$\tilde{p}<em>k$表示无缺失值样本中第$k$类所占的比例，$\tilde{r}_v$则表示无缺失值样本在属性$a$上取值$a^v$的样本所占的比例。显然$\sum^{|\mathbb{Y}|}</em>{k=1}\tilde{p}<em>k=1,\sum^V</em>{v=1}\tilde{r}_v=1$。</p><p>基于上述定义，信息增益计算式可推广为</p><p>$$<br>Gain(D, a) = \rho \times Gain(\tilde{D}, a)<br>\<br>= \rho \times { Ent(\tilde{D}) - \sum^V_{v=1}\tilde{r}_vEnt(\tilde{D}^v) }<br>$$</p><p>其中</p><p>$$<br>Ent(\tilde{D}) = - \sum^{|\mathbb{Y}|}_{k=1}\tilde{p}_k\log_2\tilde{p}_k<br>$$</p><hr><p>对问题（2）,若样本$\boldsymbol{x}$在划分属性$a$上的取值已知，则将$\boldsymbol{x}$划入与其取值对应的子结点，且样本权值保持为$w_{\boldsymbol{x}}$。<br>若样本$\boldsymbol{x}$在划分属性$a$上的取值未知，则将$\boldsymbol{x}$同时划入所有子结点，且样本权值在与属性$a^v$对应的子结点中调整为$\tilde{r} \cdot w_{\boldsymbol{x}}$；<br>直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。</p><h2 id="5-多变量决策树"><a href="#5-多变量决策树" class="headerlink" title="5. 多变量决策树"></a>5. 多变量决策树</h2><p>使用单属性作为划分结点，我们得到的决策树的分类边界是平行于该属性对应的轴。</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/tree1.png" alt="tree1.png"></p><p>但在现实任务中，我们希望分类边界尽可能平滑，越平滑意味着每个结点包含的分类标准越复杂，需要的属性组合就越多，所以我们希望能以局部线性拟合平滑曲线，这就是多变量决策树</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/tree3.png" alt="tree3.png"></p><p>在多变量决策树中，每个非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试；换言之，每个非叶结点是一个形如$\sum^d_{i=1}w_ia_i=t$的线性分类器，其中$w_i$是属性$a_i$的权重，$w_i,t$可在该结点所含的样本集和属性集上学得。于是，多变量决策树的训练过程就不是寻找最优划分属性，而是建立一个合适的线性分类器。</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/tree2.png" alt="tree2.png"></p><h2 id="6-回归树"><a href="#6-回归树" class="headerlink" title="6. 回归树"></a>6. 回归树</h2><p><img src="http://114.116.9.65:7777/images/2020/01/18/0.png" alt="0.png"></p><p>分类决策树处理的是分类问题，其特点是target是label标签，而标签是一个有限集，因此决策树的构建需要在叶结点判断标签，最终也只需要判断准确率即可，分类节点使用信息增益相关的算法即可；<br>回归树处理的是回归问题，即最终的目标是一个预测值，而我们的训练数据的真实值可以说是无穷多的，因此只能用误差损失来衡量，比如用平方误差衡量单个样本的误差。而每个结点（不只是叶结点）都对应一个预测值，这个预测值的大小是被分配到这个结点的所有样本的真实值的均值，如何选择最优属性以及最优划分阈值可以参考上面<code>4. 连续与缺失值</code>。</p><p><img src="http://114.116.9.65:7777/images/2020/01/18/1.png" alt="1.png"></p><h2 id="7-思考"><a href="#7-思考" class="headerlink" title="7. 思考"></a>7. 思考</h2><ul><li>决策树是最接近人类思考的最简单的模型，讲道理在大多数情况下，我们所做的每一个行为都是基于各种细小属性的决策，那么是否有一种复杂多变量决策树可以动态的描述一个人或者生物的行为呢，与此同时，每一个分类结点的权重也是一个变量，其收到其他因素的影响。显然，这种想法会导致维度爆炸。</li><li>换一种思路，我们基于某种主干决策树做大多数决策，同时存在偏差决策树，偏差决策树构成森林，根据主干与偏差的相互叠加，我们完成其他的行为决策。</li><li>基于决策树的其他算法框架在分类任务中具有极好的效果，比如<code>XGBoost</code>、<code>GBDT</code>、<code>Random Forest</code>，接下来会思考这些算法。</li></ul></div><div class="popular-posts-header">推荐文章</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/7d1dcda7.html" rel="bookmark">模型评估与选择</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/280b588e.html" rel="bookmark">支持向量机</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/7ca31f7.html" rel="bookmark">神经网络</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/fcb2659b.html" rel="bookmark">数据集</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/archives/5206f73f.html" rel="bookmark">线性模型</a></div></li></ul><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Tao Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://zhoutao822.coding.me/archives/8ddc7426.html" title="决策树">http://zhoutao822.coding.me/archives/8ddc7426.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Theory/" rel="tag"># Theory</a> <a href="/tags/Decision-Tree/" rel="tag"># Decision Tree</a> <a href="/tags/Regression-Decision-Tree/" rel="tag"># Regression Decision Tree</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/5206f73f.html" rel="prev" title="线性模型"><i class="fa fa-chevron-left"></i> 线性模型</a></div><div class="post-nav-item"><a href="/archives/7ca31f7.html" rel="next" title="神经网络">神经网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-基本流程"><span class="nav-text">1. 基本流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-划分选择"><span class="nav-text">2. 划分选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-信息增益"><span class="nav-text">2.1 信息增益</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-增益率"><span class="nav-text">2.2 增益率</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-基尼指数"><span class="nav-text">2.3 基尼指数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-剪枝处理"><span class="nav-text">3. 剪枝处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-预剪枝"><span class="nav-text">3.1 预剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-后剪枝"><span class="nav-text">3.2 后剪枝</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-连续与缺失值"><span class="nav-text">4. 连续与缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-连续值处理"><span class="nav-text">4.1 连续值处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-缺失值处理"><span class="nav-text">4.2 缺失值处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-多变量决策树"><span class="nav-text">5. 多变量决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-回归树"><span class="nav-text">6. 回归树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-思考"><span class="nav-text">7. 思考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Tao Zhou" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Tao Zhou</p><div class="site-description" itemprop="description">学习笔记</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">61</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">126</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Tao Zhou</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">897k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">13:36</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0</div><script>function leancloudSelector(url) {
    url = encodeURI(url);
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.getAttribute('id'));
      var title = visitors.getAttribute('data-flag-title');

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
              leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .catch(error => {
                console.error('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.getAttribute('id'));
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (let item of results) {
            let { url, time } = item;
            leancloudSelector(url).innerText = time;
          }
          for (let url of entries) {
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
            'X-LC-Key': 'xoukWFyqvFIXJ6DfxLLYtsTP',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'iyNzyQRx6yCU5YQVEXPl0hSe-gzGzoHsz',
      appKey: 'xoukWFyqvFIXJ6DfxLLYtsTP',
      placeholder: "Just go go",
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: false,
      lang: 'zh-cn' || 'zh-cn',
      path: location.pathname,
      recordIP: true,
      serverURLs: ''
    });
  }, window.Valine);
});</script></body></html><!-- rebuild by neat -->